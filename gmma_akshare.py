import streamlit as st
import akshare as ak
import pandas as pd
import plotly.graph_objects as go
import numpy as np
from datetime import datetime, timedelta
import time
from functools import lru_cache
import os
import json
import concurrent.futures
import gc

# Set page layout to wide mode
st.set_page_config(
    page_title="GMMA è‚¡ç¥¨åˆ†æå·¥å…·",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# App title and description
st.title("é¡¾æ¯”å¤šé‡ç§»åŠ¨å¹³å‡çº¿ (GMMA) å›¾è¡¨")
st.markdown("""
æ­¤åº”ç”¨ç¨‹åºæ˜¾ç¤ºä½¿ç”¨ akshare æ•°æ®çš„ä¸­å›½ A è‚¡è‚¡ç¥¨çš„å¤æ™®åˆ©å¤šé‡ç§»åŠ¨å¹³å‡çº¿ (GMMA) å›¾è¡¨ã€‚  
å¯ä»¥åˆ†æå•ä¸ªè‚¡ç¥¨æˆ–è‡ªåŠ¨æ‰«ææœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚
""")

# Cache directory setup
CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# Function to load stock data with caching
@st.cache_data(ttl=300)  # Cache for 1 hour
def fetch_stock_data(ticker, start_date, end_date):
    """Fetch stock data with caching to avoid repeated API calls"""
    try:
        # Fetch stock data using akshare
        stock_data = ak.stock_zh_a_hist(symbol=ticker, period="daily", 
                                     start_date=start_date, end_date=end_date, adjust="")
        if stock_data.empty:
            return None
            
        # Rename columns and process data
        stock_data.rename(columns={'æ—¥æœŸ': 'date', 'æ”¶ç›˜': 'close', 'å¼€ç›˜': 'open'}, inplace=True)
        stock_data['date'] = pd.to_datetime(stock_data['date'])
        stock_data.set_index('date', inplace=True)
        stock_data.sort_index(inplace=True)
        
        return stock_data
    except Exception as e:
        print(f"Error fetching {ticker}: {str(e)}")
        return None

# Function to check if a stock has a recent crossover (optimized)
def has_recent_crossover(ticker, days_to_check=3, history_days=365):
    try:
        # Calculate date range for the past history_days (enough data to calculate EMAs)
        end_date = datetime.today().strftime('%Y%m%d')
        # Get more historical data - add 30% more days for proper EMA calculation
        calculation_days = int(history_days * 1.3)
        start_date = (datetime.today() - timedelta(days=calculation_days)).strftime('%Y%m%d')
        
        # Fetch stock data using cached function
        stock_data = fetch_stock_data(ticker, start_date, end_date)
        if stock_data is None or stock_data.empty:
            return False, None
            
        # Calculate EMAs
        for period in [3, 5, 8, 10, 12, 15, 30, 35, 40, 45, 50, 60]:
            stock_data[f"EMA{period}"] = stock_data["close"].ewm(span=period, adjust=False).mean()
        
        # Calculate average EMAs
        short_terms = [3, 5, 8, 10, 12, 15]
        long_terms = [30, 35, 40, 45, 50, 60]
        stock_data['avg_short_ema'] = stock_data[[f'EMA{period}' for period in short_terms]].mean(axis=1)
        stock_data['avg_long_ema'] = stock_data[[f'EMA{period}' for period in long_terms]].mean(axis=1)
        
        # Detect crossovers
        stock_data['short_above_long'] = stock_data['avg_short_ema'] > stock_data['avg_long_ema']
        stock_data['crossover'] = False
        
        # Find crossover points - Use loc[] instead of chained assignment
        for i in range(1, len(stock_data)):
            if not stock_data['short_above_long'].iloc[i-1] and stock_data['short_above_long'].iloc[i]:
                stock_data.loc[stock_data.index[i], 'crossover'] = True
        
        # Check if there's a crossover in the last 'days_to_check' days
        recent_data = stock_data.iloc[-days_to_check:]
        has_crossover = recent_data['crossover'].any()
        
        # If no crossover, return early and free memory
        if not has_crossover:
            return False, None
            
        return has_crossover, stock_data
    except Exception as e:
        print(f"Error checking {ticker}: {str(e)}")
        return False, None

# Add a caching mechanism for expensive API calls with local file support
def fetch_industry_data():
    """Fetch and cache all industry data, using local file when possible"""
    try:
        # Find the most recent industry cache file
        cache_files = [f for f in os.listdir(CACHE_DIR) if f.startswith('industry_data_') and f.endswith('.json')]
        latest_file = None
        is_cache_valid = False
        
        if cache_files:
            # Get the most recent file
            cache_files.sort(reverse=True)  # Sort by filename (which includes date)
            latest_file = os.path.join(CACHE_DIR, cache_files[0])
            
            # Extract date from filename (industry_data_YYYY-MM-DD.json)
            try:
                file_date_str = cache_files[0].replace('industry_data_', '').replace('.json', '')
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                # Check if file is less than 2 months old
                is_cache_valid = (datetime.now() - file_date).days < 60
            except:
                is_cache_valid = False
        
        # Load from cache file if valid
        if is_cache_valid and latest_file and os.path.exists(latest_file):
            progress_text = st.empty()
            progress_text.text("ä»æœ¬åœ°ç¼“å­˜åŠ è½½è¡Œä¸šæ•°æ®...")
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            progress_text.empty()
            return cached_data
        
        # If cache is invalid or doesn't exist, fetch fresh data
        progress_text = st.empty()
        progress_text.text("æ­£åœ¨ä»æœåŠ¡å™¨è·å–è¡Œä¸šæ•°æ®...")
        
        # Get all industry names
        industry_df = ak.stock_board_industry_name_em()
        industry_list = industry_df["æ¿å—åç§°"].tolist()
        
        # Create dictionaries to store industry data
        industry_stocks = {}  # Industry -> List of stocks
        industry_counts = {}  # Industry -> Count of stocks
        stock_to_industry = {}  # Stock code -> Industry
        
        # Get stock data for each industry
        for i, industry in enumerate(industry_list):
            progress_text.text(f"æ­£åœ¨è·å–è¡Œä¸šæ•°æ®: {i+1}/{len(industry_list)} - {industry}")
            try:
                # Fetch stocks in this industry
                industry_stocks_df = ak.stock_board_industry_cons_em(symbol=industry)
                if not industry_stocks_df.empty:
                    # Process stocks in this industry
                    stocks_list = []
                    for _, row in industry_stocks_df.iterrows():
                        stock_code = row["ä»£ç "].split('.')[0].zfill(6)
                        stock_name = row["åç§°"]
                        stocks_list.append((stock_code, stock_name))
                        # Map stock code to industry
                        stock_to_industry[stock_code] = industry
                    
                    # Store data
                    industry_stocks[industry] = stocks_list
                    industry_counts[industry] = len(stocks_list)
                else:
                    industry_stocks[industry] = []
                    industry_counts[industry] = 0
            except Exception as e:
                print(f"Error fetching stocks for {industry}: {str(e)}")
                industry_stocks[industry] = []
                industry_counts[industry] = 0
        
        # Prepare data structure for caching
        industry_data = {
            "industry_list": industry_list,
            "industry_stocks": industry_stocks,
            "industry_counts": industry_counts,
            "stock_to_industry": stock_to_industry,
            "fetch_date": datetime.now().strftime('%Y-%m-%d')
        }
        
        # Save to a new cache file with current date
        current_date = datetime.now().strftime('%Y-%m-%d')
        cache_file = os.path.join(CACHE_DIR, f'industry_data_{current_date}.json')
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(industry_data, f, ensure_ascii=False, indent=2)
        
        # Clean up old cache files (keep only the most recent 3)
        cache_files = [f for f in os.listdir(CACHE_DIR) if f.startswith('industry_data_') and f.endswith('.json')]
        if len(cache_files) > 3:
            cache_files.sort()  # Sort by date ascending
            for old_file in cache_files[:-3]:  # Remove all but the 3 most recent
                try:
                    os.remove(os.path.join(CACHE_DIR, old_file))
                except:
                    pass
        
        progress_text.empty()
        return industry_data
    
    except Exception as e:
        st.error(f"è·å–è¡Œä¸šæ•°æ®å¤±è´¥: {str(e)}")
        return {
            "industry_list": [],
            "industry_stocks": {},
            "industry_counts": {},
            "stock_to_industry": {},
            "fetch_date": datetime.now().strftime('%Y-%m-%d')
        }

# Process batch of stocks to identify crossovers
def process_stock_batch(stock_batch, days_to_check, industry_mapping=None, display_days=365):
    crossover_stocks = []
    
    for ticker, name in stock_batch:
        # Skip stocks with special prefixes 
        if ticker.startswith(('688', '300', '8', '4')):
            continue
            
        # Check for crossover
        has_crossover, stock_data = has_recent_crossover(ticker, days_to_check, history_days=display_days)
        
        if has_crossover:
            # Get industry information for the stock
            industry = industry_mapping.get(ticker, "æœªçŸ¥") if industry_mapping else "æœªçŸ¥"
            
            # Include industry in the crossover_stocks list
            crossover_stocks.append((ticker, name, industry, stock_data))
    
    return crossover_stocks

# Sidebar options
st.sidebar.title("åˆ†ææ¨¡å¼")
analysis_mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["è‡ªåŠ¨æ‰«æä¹°å…¥ä¿¡å·","å•ä¸€è‚¡ç¥¨åˆ†æ"])

if analysis_mode == "å•ä¸€è‚¡ç¥¨åˆ†æ":
    # Single stock analysis mode - similar to the original code
    st.sidebar.title("è‚¡ç¥¨è¾“å…¥")
    ticker = st.sidebar.text_input("è¾“å…¥ 6 ä½è‚¡ç¥¨ä»£ç ï¼ˆä¾‹å¦‚ï¼Œ000001ï¼‰", "000001")
    
    st.sidebar.title("æ˜¾ç¤ºé€‰é¡¹")
    show_short_term = st.sidebar.checkbox("æ˜¾ç¤ºçŸ­æœŸ EMA", value=True)
    show_long_term = st.sidebar.checkbox("æ˜¾ç¤ºé•¿æœŸ EMA", value=True)
    
    # Add history length selection dropdown
    history_options = {
        "10 å¹´": 3650,
        "5 å¹´": 1825,
        "2 å¹´": 730,
        "12 ä¸ªæœˆ": 365,
        "6 ä¸ªæœˆ": 180,
        "3 ä¸ªæœˆ": 90
    }
    selected_history = st.sidebar.selectbox(
        "å†å²æ•°æ®é•¿åº¦",
        options=list(history_options.keys()),
        index=3  # Default to 12 months
    )
    
    # Calculate date range based on selected period
    end_date = datetime.today().strftime('%Y%m%d')
    # Set display period to the selected period
    display_days = history_options[selected_history]
    # Add extra time for calculation (30% more) to ensure proper EMA calculation
    calculation_days = int(display_days * 1.3)
    
    display_start_date = (datetime.today() - timedelta(days=display_days)).strftime('%Y%m%d')
    calculation_start_date = (datetime.today() - timedelta(days=calculation_days)).strftime('%Y%m%d')
    
    # Fetch and process stock data
    with st.spinner("è·å–æ•°æ®ä¸­..."):
        try:
            # Remove exchange suffix if present (e.g., '000001.SZ' -> '000001')
            ticker = ticker.split('.')[0]
            if not ticker.isdigit() or len(ticker) != 6:
                st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„ 6 ä½è‚¡ç¥¨ä»£ç ã€‚")
            else:
                # Fetch stock data using akshare with extended history for proper EMA calculation
                stock_data = fetch_stock_data(ticker, calculation_start_date, end_date)
                if stock_data is None or stock_data.empty:
                    st.error("æœªæ‰¾åˆ°æ‰€è¾“å…¥è‚¡ç¥¨ä»£ç çš„æ•°æ®ã€‚è¯·æ£€æŸ¥ä»£ç å¹¶é‡è¯•ã€‚")
                else:
                    # Calculate Exponential Moving Averages (EMAs)
                    for period in [3, 5, 8, 10, 12, 15, 30, 35, 40, 45, 50, 60]:
                        stock_data[f"EMA{period}"] = stock_data["close"].ewm(span=period, adjust=False).mean()
                    
                    # Define short-term and long-term EMAs
                    short_terms = [3, 5, 8, 10, 12, 15]
                    long_terms = [30, 35, 40, 45, 50, 60]
                    
                    # Calculate average of short-term and long-term EMAs for each day
                    stock_data['avg_short_ema'] = stock_data[[f'EMA{period}' for period in short_terms]].mean(axis=1)
                    stock_data['avg_long_ema'] = stock_data[[f'EMA{period}' for period in long_terms]].mean(axis=1)
                    
                    # Detect crossovers (short-term crossing above long-term)
                    stock_data['short_above_long'] = stock_data['avg_short_ema'] > stock_data['avg_long_ema']
                    stock_data['crossover'] = False
                    
                    # Find the exact crossover points (when short_above_long changes from False to True)
                    for i in range(1, len(stock_data)):
                        if not stock_data['short_above_long'].iloc[i-1] and stock_data['short_above_long'].iloc[i]:
                            stock_data.loc[stock_data.index[i], 'crossover'] = True
                    
                    # Only display the last 6 months in the chart
                    display_date = pd.to_datetime(display_start_date)
                    display_data = stock_data[stock_data.index >= display_date]
                    
                    # Create Plotly figure
                    fig = go.Figure()
                    
                    # Add candlestick chart
                    fig.add_trace(go.Candlestick(
                        x=display_data.index,
                        open=display_data["open"],
                        high=display_data[["open", "close"]].max(axis=1),
                        low=display_data[["open", "close"]].min(axis=1),
                        close=display_data["close"],
                        increasing_line_color='green',
                        decreasing_line_color='red',
                        name="Price"
                    ))
                    
                    # Add short-term EMAs (blue)
                    if show_short_term:
                        for i, period in enumerate([3, 5, 8, 10, 12, 15]):
                            fig.add_trace(go.Scatter(
                                x=display_data.index,
                                y=display_data[f"EMA{period}"],
                                mode="lines",
                                name=f"EMA{period}",
                                line=dict(color="skyblue", width=1),
                                legendgroup="short_term",
                                showlegend=(i == 0)
                            ))
                    
                    # Add long-term EMAs (red)
                    if show_long_term:
                        for i, period in enumerate([30, 35, 40, 45, 50, 60]):
                            fig.add_trace(go.Scatter(
                                x=display_data.index,
                                y=display_data[f"EMA{period}"],
                                mode="lines",
                                name=f"EMA{period}",
                                line=dict(color="lightcoral", width=1),
                                legendgroup="long_term",
                                showlegend=(i == 0)
                            ))
                    
                    # Add average short-term and long-term EMAs to visualize crossover
                    fig.add_trace(go.Scatter(
                        x=display_data.index,
                        y=display_data['avg_short_ema'],
                        mode="lines",
                        name="Avg Short-term EMAs",
                        line=dict(color="blue", width=2, dash='dot'),
                    ))
                    
                    fig.add_trace(go.Scatter(
                        x=display_data.index,
                        y=display_data['avg_long_ema'],
                        mode="lines",
                        name="Avg Long-term EMAs",
                        line=dict(color="red", width=2, dash='dot'),
                    ))
                    
                    # Mark crossover signals on the chart
                    crossover_dates = display_data[display_data['crossover']].index
                    for date in crossover_dates:
                        price_at_crossover = display_data.loc[date, 'close']
                        # Add vertical line at crossover
                        fig.add_shape(
                            type="line",
                            x0=date,
                            y0=price_at_crossover * 0.97,
                            x1=date,
                            y1=price_at_crossover * 1.03,
                            line=dict(color="green", width=3),
                        )
                        # Add annotation
                        fig.add_annotation(
                            x=date,
                            y=price_at_crossover * 1.04,
                            text="ä¹°å…¥ä¿¡å·",
                            showarrow=True,
                            arrowhead=1,
                            arrowcolor="green",
                            arrowsize=1,
                            arrowwidth=2,
                            font=dict(color="green", size=12)
                        )
                    
                    # Count and display the number of signals
                    signal_count = len(crossover_dates)
                    if signal_count > 0:
                        last_signal = crossover_dates[-1].strftime('%Y-%m-%d') if signal_count > 0 else "None"
                        signal_info = f"**ä¹°å…¥ä¿¡å·**: å…± {signal_count} ä¸ª, æœ€è¿‘ä¿¡å·æ—¥æœŸ: {last_signal}"
                        fig.add_annotation(
                            x=0.02,
                            y=0.98,
                            xref="paper",
                            yref="paper",
                            text=signal_info,
                            showarrow=False,
                            font=dict(size=14, color="green"),
                            bgcolor="white",
                            bordercolor="green",
                            borderwidth=1,
                            align="left"
                        )
                    
                    # Customize plot layout
                    fig.update_layout(
                        title=f"è‚¡ç¥¨ {ticker} GMMA å›¾è¡¨ ({selected_history} å†å²æ•°æ®, æ ‡è®°: çŸ­æœŸEMAä»ä¸‹æ–¹ç©¿è¿‡é•¿æœŸEMA)",
                        xaxis_title="æ—¥æœŸ",
                        yaxis_title="ä»·æ ¼",
                        legend_title="å›¾ä¾‹",
                        hovermode="x unified",
                        template="plotly_white"
                    )
                    
                    # Display the plot in Streamlit
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Display crossover days in a table
                    if len(crossover_dates) > 0:
                        st.subheader("ä¹°å…¥ä¿¡å·æ—¥æœŸ")
                        signal_df = pd.DataFrame(crossover_dates, columns=["æ—¥æœŸ"])
                        signal_df["æ—¥æœŸ"] = signal_df["æ—¥æœŸ"].dt.strftime('%Y-%m-%d')
                        st.table(signal_df)
        except Exception as e:
            st.error(f"è·å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")

else:  # Auto scan mode
    st.sidebar.title("æ‰«æè®¾ç½®")
    days_to_check = st.sidebar.slider("æ£€æŸ¥æœ€è¿‘å‡ å¤©å†…çš„ä¿¡å·", 1, 7, 1)
    max_stocks = st.sidebar.slider("æœ€å¤šæ˜¾ç¤ºè‚¡ç¥¨æ•°é‡", 1, 500, 200)
    batch_size = st.sidebar.slider("æ‰¹å¤„ç†å¤§å° (è¾ƒå°å€¼æ›´çœå†…å­˜)", 10, 100, 50)
    
    # Add history length selection dropdown for chart display
    history_options = {
        "10 å¹´": 3650,
        "5 å¹´": 1825,
        "2 å¹´": 730,
        "12 ä¸ªæœˆ": 365,
        "6 ä¸ªæœˆ": 180,
        "3 ä¸ªæœˆ": 90
    }
    selected_history = st.sidebar.selectbox(
        "å†å²æ•°æ®é•¿åº¦",
        options=list(history_options.keys()),
        index=3  # Default to 12 months
    )
    display_days = history_options[selected_history]
    
    # Add more filtering options
    st.sidebar.title("ç­›é€‰é€‰é¡¹")
    exclude_st = st.sidebar.checkbox("æ’é™¤STè‚¡ç¥¨", value=False)
    exclude_new = st.sidebar.checkbox("æ’é™¤ä¸Šå¸‚ä¸è¶³3ä¸ªæœˆçš„è‚¡ç¥¨", value=False)
    exclude_688 = st.sidebar.checkbox("æ’é™¤ç§‘åˆ›æ¿è‚¡ç¥¨ (688å¼€å¤´)", value=False)
    exclude_300 = st.sidebar.checkbox("æ’é™¤åˆ›ä¸šæ¿è‚¡ç¥¨ (300å¼€å¤´)", value=False)
    
    # Add industry selection option
    scan_mode = st.sidebar.radio("æ‰«æèŒƒå›´", ["æŒ‰è¡Œä¸šæ¿å—","å…¨éƒ¨ A è‚¡"])
    
    selected_industry = None
    
    # Industry board selection
    if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
        try:
            # Fetch all industry data once (cached)
            with st.spinner("è·å–è¡Œä¸šæ¿å—æ•°æ®..."):
                industry_data = fetch_industry_data()
                industry_list = industry_data["industry_list"]
                industry_counts = industry_data["industry_counts"]
                industry_stocks = industry_data["industry_stocks"]
                
                # Get fresh industry price change data directly from API
                fresh_industry_df = ak.stock_board_industry_name_em()
                industry_change_pct = {row["æ¿å—åç§°"]: row["æ¶¨è·Œå¹…"] 
                                       for _, row in fresh_industry_df.iterrows()}
            
            # Remove loading message from sidebar
            st.sidebar.text("")
            
            # Format options to include stock count and æ¶¨è·Œå¹…: "è¡Œä¸šå (123è‚¡) â†‘2.50%"
            industry_options = []
            for ind in industry_list:
                count = industry_counts[ind]
                # Get the latest change percentage for this industry
                change_pct = industry_change_pct.get(ind, 0)
                
                # Format percentage with arrow indicator and color
                if change_pct > 0:
                    pct_str = f"â†‘{change_pct:.2f}%"
                elif change_pct < 0:
                    pct_str = f"â†“{abs(change_pct):.2f}%"
                else:
                    pct_str = f"0.00%"
                    
                # Create the formatted option
                option = f"{ind} ({count}è‚¡) {pct_str}"
                industry_options.append(option)
            
            # Create a mapping from formatted name back to original name
            industry_name_mapping = {option: ind for ind, option in zip(industry_list, industry_options)}
            
            # Get fresh industry order directly from the API for sorting
            try:
                ordered_industry_list = fresh_industry_df["æ¿å—åç§°"].tolist()
                
                # Create a mapping for sorting based on the original order
                industry_order = {ind: idx for idx, ind in enumerate(ordered_industry_list)}
                
                # Sort industry_options based on the original order
                industry_options_sorted = sorted(
                    industry_options,
                    key=lambda x: industry_order.get(industry_name_mapping[x], float('inf'))
                )
            except Exception as e:
                # Fallback to unsorted if API call fails
                st.sidebar.warning(f"æ— æ³•è·å–è¡Œä¸šæ’åºï¼Œå°†ä½¿ç”¨é»˜è®¤é¡ºåº: {str(e)}")
                industry_options_sorted = industry_options
            
            # Use the sorted options in the multiselect
            default_option = industry_options_sorted[0] if industry_options_sorted else None
            selected_industry_options = st.sidebar.multiselect(
                "é€‰æ‹©è¡Œä¸šæ¿å— (å¯å¤šé€‰)",
                options=industry_options_sorted,
                default=[default_option] if default_option else []
            )
            
            # Convert the selected formatted options back to original industry names
            selected_industries = [industry_name_mapping[opt] for opt in selected_industry_options]
            
            # Show the selected industry info
            if selected_industries:
                total_stocks = sum(industry_counts[ind] for ind in selected_industries)
                industries_text = ", ".join(selected_industries)
                st.sidebar.info(f"å·²é€‰æ‹©: {industries_text}\n\nå…±è®¡çº¦ {total_stocks} åªè‚¡ç¥¨")
        except Exception as e:
            st.sidebar.error(f"è·å–è¡Œä¸šæ¿å—å¤±è´¥: {str(e)}")

    if st.sidebar.button("å¼€å§‹æ‰«æ"):
        with st.spinner("æ­£åœ¨æ‰«æä¹°å…¥ä¿¡å·ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."):
            try:
                # Variable to track if we have valid stocks to scan
                have_stocks_to_scan = True
                
                # Get stock list based on scan mode
                if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" and selected_industries:
                    # Create an empty DataFrame to store all industry stocks
                    all_industry_stocks_list = []
                    
                    # Use cached data instead of making new API calls
                    for industry in selected_industries:
                        if industry in industry_stocks:
                            all_industry_stocks_list.extend(industry_stocks[industry])
                    
                    if all_industry_stocks_list:
                        # Convert to DataFrame
                        stock_info_df = pd.DataFrame(all_industry_stocks_list, columns=["code", "name"])
                        # Remove duplicates
                        stock_info_df = stock_info_df.drop_duplicates(subset=["code"])
                    else:
                        st.error("æœªèƒ½è·å–æ‰€é€‰è¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨ã€‚")
                        have_stocks_to_scan = False
                else:
                    # Get all A-share stock codes and names
                    stock_info_df = ak.stock_info_a_code_name()
                
                # Only proceed if we have stocks to scan
                if have_stocks_to_scan:
                    # Apply filtering based on user preferences
                    if exclude_st:
                        stock_info_df = stock_info_df[~stock_info_df["name"].str.contains("ST", case=False)]
                    
                    # Apply code-based filtering
                    if exclude_688:
                        stock_info_df = stock_info_df[~stock_info_df["code"].str.startswith("688")]
                    
                    if exclude_300:
                        stock_info_df = stock_info_df[~stock_info_df["code"].str.startswith("300")]
                    
                    # Show how many stocks will be scanned
                    stock_count = len(stock_info_df)
                    st.info(f"å‡†å¤‡æ‰«æ {stock_count} åªè‚¡ç¥¨ï¼Œåˆ†æ‰¹å¤„ç†ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨...")
                    
                    # Create UI elements for real-time updates
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    results_count = st.empty()
                    
                    # Create containers for results that will be updated with each batch
                    st.subheader("ä¹°å…¥ä¿¡å·è‚¡ç¥¨åˆ—è¡¨")
                    table_container = st.empty()
                    
                    # Create an expander section for all charts
                    st.subheader("GMMA å›¾è¡¨ (å®æ—¶æ›´æ–°)")
                    charts_container = st.container()
                    
                    # Counter for stocks with crossover
                    crossover_stocks = []
                    
                    # If in industry mode, map stock codes to their industries
                    industry_mapping = industry_data["stock_to_industry"] if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" else {}
                    
                    # Convert DataFrame to list for batch processing
                    stocks_to_scan = list(zip(stock_info_df["code"], stock_info_df["name"]))
                    
                    # Calculate total number of batches
                    num_batches = (len(stocks_to_scan) + batch_size - 1) // batch_size
                    
                    # Process stocks in batches to limit memory usage
                    for batch_idx in range(num_batches):
                        # Update progress and status
                        progress_bar.progress(min(batch_idx / num_batches, 1.0))
                        batch_start = batch_idx * batch_size
                        batch_end = min((batch_idx + 1) * batch_size, len(stocks_to_scan))
                        current_batch = stocks_to_scan[batch_start:batch_end]
                        
                        status_text.text(f"å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{num_batches}ï¼Œè‚¡ç¥¨ {batch_start+1}-{batch_end}/{stock_count}")
                        
                        # Process current batch
                        batch_results = process_stock_batch(current_batch, days_to_check, industry_mapping, display_days)
                        
                        # If we found new stocks with crossovers
                        if batch_results:
                            # Add new results to our master list
                            crossover_stocks.extend(batch_results)
                            
                            # Update the results counter
                            results_count.info(f"å·²æ‰¾åˆ° {len(crossover_stocks)} åªå‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨")
                            
                            # Update summary table with all found stocks
                            summary_df = pd.DataFrame(
                                [(t, n, ind) for t, n, ind, _ in crossover_stocks], 
                                columns=["ä»£ç ", "åç§°", "æ‰€å±è¡Œä¸š"]
                            )
                            table_container.dataframe(summary_df)
                            
                            # Display charts for new stocks in this batch
                            with charts_container:
                                for ticker, name, industry, stock_data in batch_results:
                                    with st.expander(f"{ticker} - {name} ({industry})", expanded=True):
                                        # Create GMMA chart
                                        fig = go.Figure()
                                        
                                        # Filter to display based on selected history length
                                        display_start_date = (datetime.today() - timedelta(days=display_days))
                                        display_data = stock_data[stock_data.index >= display_start_date]
                                        
                                        # Add candlestick chart
                                        fig.add_trace(go.Candlestick(
                                            x=display_data.index,
                                            open=display_data["open"],
                                            high=display_data[["open", "close"]].max(axis=1),
                                            low=display_data[["open", "close"]].min(axis=1),
                                            close=display_data["close"],
                                            increasing_line_color='red',
                                            decreasing_line_color='green',
                                            name="Price"
                                        ))
                                        
                                        # Add short-term EMAs (blue)
                                        for i, period in enumerate([3, 5, 8, 10, 12, 15]):
                                            fig.add_trace(go.Scatter(
                                                x=display_data.index,
                                                y=display_data[f"EMA{period}"],
                                                mode="lines",
                                                name=f"EMA{period}",
                                                line=dict(color="skyblue", width=1),
                                                legendgroup="short_term",
                                                showlegend=(i == 0)
                                            ))
                                        
                                        # Add long-term EMAs (red)
                                        for i, period in enumerate([30, 35, 40, 45, 50, 60]):
                                            fig.add_trace(go.Scatter(
                                                x=display_data.index,
                                                y=display_data[f"EMA{period}"],
                                                mode="lines",
                                                name=f"EMA{period}",
                                                line=dict(color="lightcoral", width=1),
                                                legendgroup="long_term",
                                                showlegend=(i == 0)
                                            ))
                                        
                                        # Add average EMAs
                                        fig.add_trace(go.Scatter(
                                            x=display_data.index,
                                            y=display_data['avg_short_ema'],
                                            mode="lines",
                                            name="Avg Short-term EMAs",
                                            line=dict(color="blue", width=2, dash='dot'),
                                        ))
                                        
                                        fig.add_trace(go.Scatter(
                                            x=display_data.index,
                                            y=display_data['avg_long_ema'],
                                            mode="lines",
                                            name="Avg Long-term EMAs",
                                            line=dict(color="red", width=2, dash='dot'),
                                        ))
                                        
                                        # Mark crossover signals
                                        crossover_dates = display_data[display_data['crossover']].index
                                        for date in crossover_dates:
                                            price_at_crossover = display_data.loc[date, 'close']
                                            fig.add_annotation(
                                                x=date,
                                                y=price_at_crossover * 1.04,
                                                text="ä¹°å…¥ä¿¡å·",
                                                showarrow=True,
                                                arrowhead=1,
                                                arrowcolor="green",
                                                arrowsize=1,
                                                arrowwidth=2,
                                                font=dict(color="green", size=12)
                                            )
                                        
                                        # Layout
                                        fig.update_layout(
                                            title=f"{ticker} - {name} GMMA å›¾è¡¨ ({selected_history} å†å²æ•°æ®)",
                                            xaxis_title="æ—¥æœŸ",
                                            yaxis_title="ä»·æ ¼",
                                            legend_title="å›¾ä¾‹",
                                            hovermode="x unified",
                                            template="plotly_white",
                                            height=800
                                        )
                                        
                                        # Display the plot
                                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Check if we found enough stocks
                        if len(crossover_stocks) >= max_stocks:
                            status_text.text(f"å·²æ‰¾åˆ°è¶³å¤Ÿæ•°é‡çš„è‚¡ç¥¨ ({max_stocks}åª)ï¼Œæå‰åœæ­¢æ‰«æ...")
                            break
                            
                        # Force garbage collection to free memory
                        gc.collect()
                    
                    # Final update
                    progress_bar.progress(1.0)
                    status_text.empty()
                    
                    # Final message
                    if len(crossover_stocks) == 0:
                        st.warning(f"æ²¡æœ‰æ‰¾åˆ°åœ¨æœ€è¿‘ {days_to_check} å¤©å†…å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚")
                    else:
                        scan_scope = f"æ‰€é€‰ {len(selected_industries)} ä¸ªè¡Œä¸š" if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" else "å…¨éƒ¨ A è‚¡"
                        st.success(f"æ‰«æå®Œæˆï¼šåœ¨{scan_scope}ä¸­å…±æ‰¾åˆ° {len(crossover_stocks)} åªåœ¨æœ€è¿‘ {days_to_check} å¤©å†…å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚")
                
            except Exception as e:
                st.error(f"æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
    else:
        if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
            if selected_industries:
                industry_count = len(selected_industries)
                total_stocks = sum(industry_counts.get(ind, 0) for ind in selected_industries)
                industries_text = f"{industry_count} ä¸ªè¡Œä¸š (çº¦ {total_stocks} åªè‚¡ç¥¨)" if industry_count > 1 else f"{selected_industries[0]} (çº¦ {industry_counts.get(selected_industries[0], 0)} åªè‚¡ç¥¨)"
                st.info(f"è¯·ç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ä»¥æŸ¥æ‰¾ {industries_text} ä¸­æœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚")
            else:
                st.info("è¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€ä¸ªè¡Œä¸šæ¿å—ï¼Œç„¶åç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ã€‚")
        else:
            st.info("è¯·ç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ä»¥æŸ¥æ‰¾æœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚\n\næ³¨æ„ï¼šå…¨éƒ¨Aè‚¡æ‰«æå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®å…ˆå°è¯•æŒ‰è¡Œä¸šæ¿å—æ‰«æã€‚")
