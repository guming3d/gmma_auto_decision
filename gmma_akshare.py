import json
import os
import time
from collections.abc import Mapping
from datetime import datetime, timedelta
from functools import lru_cache
from pathlib import Path

import pandas as pd
import plotly.graph_objects as go
import streamlit as st
import tushare as ts

# Set page layout to wide mode
st.set_page_config(
    page_title="GMMA è‚¡ç¥¨åˆ†æå·¥å…·",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# App title and description
st.title("é¡¾æ¯”å¤šé‡ç§»åŠ¨å¹³å‡çº¿ (GMMA) å›¾è¡¨")
st.markdown("""
æ­¤åº”ç”¨ç¨‹åºæ˜¾ç¤ºä½¿ç”¨ Tushare æ•°æ®çš„ä¸­å›½ A è‚¡è‚¡ç¥¨çš„å¤æ™®åˆ©å¤šé‡ç§»åŠ¨å¹³å‡çº¿ (GMMA) å›¾è¡¨ã€‚
å¯ä»¥åˆ†æå•ä¸ªè‚¡ç¥¨æˆ–è‡ªåŠ¨æ‰«ææœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚
""")

# ---------------------------------------------------------------------------
# Tushare helpers
# ---------------------------------------------------------------------------

def _get_tushare_token_from_secrets() -> str | None:
    """Read the Tushare token from Streamlit secrets when configured."""
    secret_token = None
    tushare_section = st.secrets.get("tushare")
    if tushare_section and isinstance(tushare_section, Mapping):
        secret_token = (
            tushare_section.get("token")
            or tushare_section.get("api_token")
            or tushare_section.get("TUSHARE_TOKEN")
        )
    return secret_token or st.secrets.get("tushare_token") or st.secrets.get(
        "TUSHARE_TOKEN"
    )


TUSHARE_TOKEN = (
    _get_tushare_token_from_secrets()
    or os.getenv("TUSHARE_TOKEN")
    or os.getenv("TS_TOKEN")
    or os.getenv("TUSHARE_PRO_TOKEN")
)
RATE_LIMIT_MSG = "æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®"
DEFAULT_MAX_RETRIES = 6
CACHE_DIR = Path(__file__).resolve().parent / "cache"
STOCK_BASIC_CACHE_PREFIX = "stock_basic"
STOCK_BASIC_CACHE_SUFFIX = ".pkl"
STOCK_BASIC_CACHE_MAX_FILES = 2
STOCK_BASIC_CACHE_TTL = timedelta(days=7)
AUTH_SESSION_KEY = "gmma_is_authenticated"
AUTH_USER_KEY = "gmma_authenticated_user"
LOGIN_FORM_KEY = "gmma_login_form"
SCAN_RESULT_CACHE_FILES = {
    "æŒ‰è¡Œä¸šæ¿å—": CACHE_DIR / "last_industry_scan.json",
    "å…¨éƒ¨ A è‚¡": CACHE_DIR / "last_all_scan.json",
}
MAX_PREVIOUS_RESULTS = 50


def _get_auth_credentials():
    """Read username/password credentials from st.secrets."""
    username = None
    password = None
    auth_section = st.secrets.get("auth")
    if auth_section and isinstance(auth_section, Mapping):
        username = auth_section.get("username") or auth_section.get("user")
        password = auth_section.get("password")

    username = (
        username or st.secrets.get("auth_username") or st.secrets.get("AUTH_USERNAME")
    )
    password = (
        password or st.secrets.get("auth_password") or st.secrets.get("AUTH_PASSWORD")
    )
    return username, password


def _trigger_rerun():
    """Trigger a Streamlit rerun using the available API."""
    rerun = getattr(st, "rerun", None) or getattr(st, "experimental_rerun", None)
    if rerun:
        rerun()


def ensure_authenticated():
    """Render a login form and block the app until authentication succeeds."""
    stored_username, stored_password = _get_auth_credentials()
    print(stored_username)
    print(stored_password)
    if not stored_username or not stored_password:
        st.error("è¯·åœ¨ st.secrets ä¸­é…ç½® auth.username å’Œ auth.passwordã€‚")
        st.stop()
    if st.session_state.get(AUTH_SESSION_KEY):
        return True
    st.subheader("ç™»å½•éªŒè¯")
    st.caption("è¯·è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ä»¥ç»§ç»­è®¿é—® GMMA å·¥å…·ã€‚")
    with st.form(LOGIN_FORM_KEY, clear_on_submit=False):
        username_input = st.text_input("ç”¨æˆ·å", key="auth_username_input")
        password_input = st.text_input(
            "å¯†ç ",
            type="password",
            key="auth_password_input",
        )
        submitted = st.form_submit_button("ç™»å½•")
    entered_username = username_input.strip()
    entered_password = password_input
    if submitted:
        if entered_username == stored_username and entered_password == stored_password:
            st.session_state[AUTH_SESSION_KEY] = True
            st.session_state[AUTH_USER_KEY] = entered_username
            st.success("ç™»å½•æˆåŠŸï¼Œæ­£åœ¨åŠ è½½åº”ç”¨â€¦â€¦")
            _trigger_rerun()
        else:
            st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚")
    st.stop()


ensure_authenticated()


def call_tushare_api(
    func,
    retries=DEFAULT_MAX_RETRIES,
    base_delay=1.2,
    backoff=1.5,
    *,
    api_name=None,
):
    """Call a Tushare API with simple backoff when hitting rate limits."""
    api_label = api_name or "unknown Tushare API"
    for attempt in range(1, retries + 1):
        try:
            return func()
        except Exception as exc:
            message = str(exc)
            is_rate_limited = RATE_LIMIT_MSG in message
            if is_rate_limited and attempt < retries:
                sleep_time = base_delay * (backoff ** (attempt - 1))
                print(
                    f"Tushare rate limit hit for {api_label}, retrying in "
                    f"{sleep_time:.1f}s (attempt {attempt}/{retries}). Error: {message}"
                )
                time.sleep(sleep_time)
                continue
            if is_rate_limited:
                print(
                    f"Tushare rate limit hit for {api_label} and retries exhausted. "
                    f"Error: {message}"
                )
            else:
                print(f"Tushare API call failed for {api_label}: {message}")
            raise


@st.cache_resource
def get_tushare_client():
    """Return a cached Tushare Pro client."""
    if not TUSHARE_TOKEN:
        raise RuntimeError("è¯·åœ¨ç¯å¢ƒå˜é‡ TUSHARE_TOKEN ä¸­é…ç½® Tushare æˆæƒä»¤ç‰Œã€‚")
    return ts.pro_api(TUSHARE_TOKEN)


def _prepare_stock_basic_df(stock_df: pd.DataFrame) -> pd.DataFrame:
    """Ensure we have a consistent schema for stock_basic data."""
    if stock_df is None or stock_df.empty:
        return pd.DataFrame()
    if "symbol" in stock_df.columns and "code" not in stock_df.columns:
        stock_df = stock_df.rename(columns={"symbol": "code"})
    if "code" not in stock_df.columns:
        raise RuntimeError("stock_basic æ•°æ®ç¼ºå°‘ code åˆ—ã€‚")
    stock_df["code"] = stock_df["code"].astype(str).str.zfill(6)
    if "name" in stock_df.columns:
        stock_df["name"] = stock_df["name"].fillna("")
    else:
        stock_df["name"] = ""
    if "ts_code" not in stock_df.columns:
        stock_df["ts_code"] = stock_df["code"].apply(to_ts_code)
    if "industry" not in stock_df.columns:
        stock_df["industry"] = ""
    if "market" not in stock_df.columns:
        stock_df["market"] = ""
    if "list_date" not in stock_df.columns:
        stock_df["list_date"] = ""
    stock_df = stock_df.drop_duplicates(subset=["code"])
    return stock_df


def _stock_basic_cache_files():
    """Return cache files sorted by most recent first."""
    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
    except OSError:
        # Directory already exists or cannot be created; proceed best-effort.
        pass
    cache_pattern = f"{STOCK_BASIC_CACHE_PREFIX}_*{STOCK_BASIC_CACHE_SUFFIX}"
    files = sorted(
        CACHE_DIR.glob(cache_pattern),
        key=lambda path: path.stat().st_mtime,
        reverse=True,
    )
    return files


def _prune_stock_basic_cache():
    """Keep only the newest STOCK_BASIC_CACHE_MAX_FILES cache entries."""
    files = _stock_basic_cache_files()
    for old_path in files[STOCK_BASIC_CACHE_MAX_FILES:]:
        try:
            old_path.unlink()
        except OSError as exc:
            print(f"æ— æ³•åˆ é™¤è¿‡æœŸçš„ stock_basic ç¼“å­˜æ–‡ä»¶ {old_path}: {exc}")


def _load_stock_basic_from_cache() -> pd.DataFrame | None:
    """Load cached stock_basic data if it is still fresh."""
    now = datetime.now()
    for path in _stock_basic_cache_files():
        try:
            mtime = datetime.fromtimestamp(path.stat().st_mtime)
        except OSError as exc:
            print(f"æ— æ³•è¯»å– stock_basic ç¼“å­˜æ–‡ä»¶ {path}: {exc}")
            continue
        if now - mtime > STOCK_BASIC_CACHE_TTL:
            continue
        try:
            cached_df = pd.read_pickle(path)
        except Exception as exc:
            print(f"è¯»å– stock_basic ç¼“å­˜æ–‡ä»¶å¤±è´¥ {path}: {exc}")
            continue
        prepared = _prepare_stock_basic_df(cached_df)
        if not prepared.empty:
            return prepared
    return None


def _save_stock_basic_cache(stock_df: pd.DataFrame):
    """Persist latest stock_basic payload and drop older snapshots."""
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    cache_path = (
        CACHE_DIR / f"{STOCK_BASIC_CACHE_PREFIX}_{timestamp}{STOCK_BASIC_CACHE_SUFFIX}"
    )
    try:
        stock_df.to_pickle(cache_path)
    except Exception as exc:
        print(f"ä¿å­˜ stock_basic ç¼“å­˜å¤±è´¥ {cache_path}: {exc}")
        return
    _prune_stock_basic_cache()


@st.cache_data(ttl=3600)
def load_stock_basic():
    """Load basic stock metadata (code, name, industry, etc.)."""
    cached_df = _load_stock_basic_from_cache()
    if cached_df is not None:
        return cached_df
    pro = get_tushare_client()
    fields = "ts_code,symbol,name,industry,market,list_date"
    stock_df = call_tushare_api(
        lambda: pro.stock_basic(
            exchange="",
            list_status="L",
            fields=fields,
        ),
        api_name="pro.stock_basic",
    )
    if stock_df.empty:
        raise RuntimeError("æ— æ³•ä» Tushare è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ã€‚")
    stock_df = _prepare_stock_basic_df(stock_df)
    _save_stock_basic_cache(stock_df)
    return stock_df


@lru_cache(maxsize=1)
def get_latest_trade_date():
    """Fetch the latest open trading date."""
    pro = get_tushare_client()
    end = datetime.today()
    start = end - timedelta(days=14)
    cal_df = call_tushare_api(
        lambda: pro.trade_cal(
            exchange="",
            start_date=start.strftime("%Y%m%d"),
            end_date=end.strftime("%Y%m%d"),
            fields="cal_date,is_open",
        ),
        api_name="pro.trade_cal",
    )
    if cal_df.empty:
        raise RuntimeError("æ— æ³•è·å–äº¤æ˜“æ—¥å†ä¿¡æ¯ã€‚")
    open_days = cal_df[cal_df["is_open"] == 1]
    if open_days.empty:
        raise RuntimeError("æœ€è¿‘ä¸¤å‘¨æ²¡æœ‰äº¤æ˜“æ—¥æ•°æ®ã€‚")
    return open_days.iloc[-1]["cal_date"]


def normalize_symbol(ticker: str) -> str:
    """Ensure ticker is a 6-digit numeric string."""
    if not ticker:
        return ""
    ticker = ticker.strip()
    if "." in ticker:
        ticker = ticker.split(".")[0]
    ticker = ticker.zfill(6)
    return ticker


def to_ts_code(symbol: str) -> str:
    """Convert 6-digit numeric code to Tushare's ts_code."""
    if not symbol:
        return ""
    if "." in symbol:
        base, suffix = symbol.split(".", 1)
        return f"{base.zfill(6)}.{suffix.upper()}"
    symbol = symbol.strip().zfill(6)
    if symbol.startswith(("6", "9")):
        market = "SH"
    elif symbol.startswith(("4", "8")):
        market = "BJ"
    else:
        market = "SZ"
    return f"{symbol}.{market}"


def fetch_daily_history(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
    """Fetch historical daily data for a ticker from Tushare."""
    pro = get_tushare_client()
    ts_code = to_ts_code(ticker)
    df = call_tushare_api(
        lambda: pro.daily(ts_code=ts_code, start_date=start_date, end_date=end_date),
        api_name="pro.daily",
    )
    if df is None or df.empty:
        return pd.DataFrame()
    df = df.rename(columns={"trade_date": "date"})
    df["date"] = pd.to_datetime(df["date"])
    df.set_index("date", inplace=True)
    df.sort_index(inplace=True)
    return df


def compute_industry_change(stock_df: pd.DataFrame) -> dict:
    """Compute latest industry percentage change based on constituent stocks."""
    try:
        pro = get_tushare_client()
        trade_date = get_latest_trade_date()
        daily_df = call_tushare_api(
            lambda: pro.daily(trade_date=trade_date, fields="ts_code,pct_chg"),
            api_name="pro.daily",
        )
        if daily_df is None or daily_df.empty:
            return {}
        merged = daily_df.merge(
            stock_df[["ts_code", "industry"]],
            on="ts_code",
            how="left",
        )
        merged = merged.dropna(subset=["industry", "pct_chg"])
        if merged.empty:
            return {}
        grouped = merged.groupby("industry")["pct_chg"].mean()
        return {industry: float(value) for industry, value in grouped.items()}
    except Exception as exc:
        print(f"Failed to compute industry change: {exc}")
        return {}


def build_industry_payload():
    """Assemble industry metadata, constituent lists, and change stats."""
    stock_df = load_stock_basic()
    industry_df = stock_df.dropna(subset=["industry"]).copy()
    industry_list = sorted(industry_df["industry"].unique().tolist())
    industry_stocks = {}
    industry_counts = {}
    for industry in industry_list:
        members = industry_df.loc[industry_df["industry"] == industry, ["code", "name"]]
        industry_stocks[industry] = members.values.tolist()
        industry_counts[industry] = len(industry_stocks[industry])
    stock_to_industry = (
        stock_df[["code", "industry"]]
        .fillna({"industry": "æœªçŸ¥"})
        .set_index("code")["industry"]
        .to_dict()
    )
    industry_change_pct = compute_industry_change(stock_df)
    return {
        "industry_list": industry_list,
        "industry_stocks": industry_stocks,
        "industry_counts": industry_counts,
        "stock_to_industry": stock_to_industry,
        "industry_change_pct": industry_change_pct,
        "fetch_date": datetime.now().strftime("%Y-%m-%d"),
    }


# Function to check if a stock has a recent crossover
def has_recent_crossover(ticker, days_to_check=3):
    try:
        ticker = normalize_symbol(ticker)
        end_date = datetime.today().strftime("%Y%m%d")
        start_date = (datetime.today() - timedelta(days=120)).strftime("%Y%m%d")
        stock_data = fetch_daily_history(ticker, start_date, end_date)
        if stock_data.empty:
            return False, None

        for period in [3, 5, 8, 10, 12, 15, 30, 35, 40, 45, 50, 60]:
            stock_data[f"EMA{period}"] = (
                stock_data["close"].ewm(span=period, adjust=False).mean()
            )

        short_terms = [3, 5, 8, 10, 12, 15]
        long_terms = [30, 35, 40, 45, 50, 60]
        stock_data["avg_short_ema"] = stock_data[
            [f"EMA{period}" for period in short_terms]
        ].mean(axis=1)
        stock_data["avg_long_ema"] = stock_data[
            [f"EMA{period}" for period in long_terms]
        ].mean(axis=1)

        stock_data["short_above_long"] = (
            stock_data["avg_short_ema"] > stock_data["avg_long_ema"]
        )
        stock_data["crossover"] = False

        for i in range(1, len(stock_data)):
            if (
                not stock_data["short_above_long"].iloc[i - 1]
                and stock_data["short_above_long"].iloc[i]
            ):
                stock_data.loc[stock_data.index[i], "crossover"] = True

        recent_data = stock_data.iloc[-days_to_check:]
        has_crossover = recent_data["crossover"].any()

        return has_crossover, stock_data if has_crossover else None
    except Exception as e:
        print(f"Error checking {ticker}: {str(e)}")
        return False, None


def display_scan_results(
    crossover_stocks,
    scan_mode,
    selected_industries,
    days_to_check,
    partial=False,
):
    """Render scan results in a consistent way."""
    if not crossover_stocks:
        if partial:
            st.warning("æ‰«ææå‰ç»ˆæ­¢ï¼Œä¸”åœ¨é”™è¯¯å‘ç”Ÿå‰æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨ã€‚")
        else:
            st.warning(f"æ²¡æœ‰æ‰¾åˆ°åœ¨æœ€è¿‘ {days_to_check} å¤©å†…å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚")
        return

    if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
        scope = (
            f"æ‰€é€‰ {len(selected_industries)} ä¸ªè¡Œä¸š"
            if selected_industries
            else "æ‰€é€‰è¡Œä¸š"
        )
    else:
        scope = "å…¨éƒ¨ A è‚¡"

    prefix = "éƒ¨åˆ†æ‰«æå®Œæˆï¼Œ" if partial else ""
    st.success(
        f"{prefix}åœ¨{scope}ä¸­æ‰¾åˆ° {len(crossover_stocks)} åªåœ¨æœ€è¿‘ {days_to_check} å¤©å†…å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚"
    )

    summary_df = pd.DataFrame(
        [(t, n, ind) for t, n, ind, _ in crossover_stocks],
        columns=["ä»£ç ", "åç§°", "æ‰€å±è¡Œä¸š"],
    )
    st.subheader("ä¹°å…¥ä¿¡å·è‚¡ç¥¨åˆ—è¡¨")
    st.table(summary_df)


def _ensure_cache_dir_exists():
    """Best-effort creation of the cache directory."""
    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
    except OSError:
        pass


def _get_scan_cache_path(scan_mode: str) -> Path | None:
    """Return the cache path for the given scan mode."""
    path = SCAN_RESULT_CACHE_FILES.get(scan_mode)
    if not path:
        return None
    _ensure_cache_dir_exists()
    return path


def save_scan_results_to_cache(
    scan_mode: str,
    selected_industries,
    days_to_check: int,
    crossover_stocks,
    *,
    partial: bool = False,
    error_message: str | None = None,
):
    """Persist the latest scan summary so it can be rendered on future runs."""
    path = _get_scan_cache_path(scan_mode)
    if not path:
        return

    results_payload = []
    for entry in crossover_stocks:
        code = name = industry = ""
        if isinstance(entry, Mapping):
            code = entry.get("code", "")
            name = entry.get("name", "")
            industry = entry.get("industry", "")
        elif isinstance(entry, (tuple, list)):
            if len(entry) > 0:
                code = entry[0]
            if len(entry) > 1:
                name = entry[1]
            if len(entry) > 2:
                industry = entry[2]
        else:
            continue

        results_payload.append(
            {
                "code": str(code),
                "name": str(name),
                "industry": str(industry),
            }
        )

    payload = {
        "mode": scan_mode,
        "timestamp": datetime.now().isoformat(),
        "days_to_check": int(days_to_check),
        "selected_industries": list(selected_industries or []),
        "results": results_payload,
        "partial": bool(partial),
        "error": (error_message or "").strip() if error_message else "",
    }

    try:
        with path.open("w", encoding="utf-8") as fp:
            json.dump(payload, fp, ensure_ascii=False, indent=2)
    except OSError as exc:
        print(f"æ— æ³•å†™å…¥æ‰«æç¼“å­˜æ–‡ä»¶ {path}: {exc}")


def load_previous_scan_results(scan_mode: str):
    """Load the latest scan summary for the requested mode."""
    path = _get_scan_cache_path(scan_mode)
    if not path or not path.exists():
        return None
    try:
        with path.open("r", encoding="utf-8") as fp:
            return json.load(fp)
    except (OSError, json.JSONDecodeError) as exc:
        print(f"è¯»å–æ‰«æç¼“å­˜æ–‡ä»¶ {path} å¤±è´¥: {exc}")
        return None


def _format_scan_timestamp(timestamp_str: str | None) -> str:
    """Turn ISO timestamps into a human readable label."""
    if not timestamp_str:
        return ""
    try:
        ts = datetime.fromisoformat(timestamp_str)
        return ts.strftime("%Y-%m-%d %H:%M")
    except ValueError:
        return timestamp_str


def render_previous_scan_card(scan_mode: str):
    """Render the card that shows the previous scan result for a mode."""
    st.markdown(f"**{scan_mode} ä¸Šæ¬¡æ‰«æ**")
    cached = load_previous_scan_results(scan_mode)
    if not cached:
        st.caption("æš‚æ— å†å²æ‰«æè®°å½•ã€‚")
        return

    timestamp_label = _format_scan_timestamp(cached.get("timestamp"))
    days = cached.get("days_to_check")
    selected_industries = cached.get("selected_industries") or []
    meta_parts = []
    if timestamp_label:
        meta_parts.append(f"è¿è¡Œæ—¶é—´: {timestamp_label}")
    if days:
        meta_parts.append(f"æœ€è¿‘ {days} å¤©")
    if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" and selected_industries:
        meta_parts.append(f"è¡Œä¸š: {', '.join(selected_industries)}")

    if meta_parts:
        st.caption(" | ".join(meta_parts))

    if cached.get("partial"):
        st.warning("ä¸Šæ¬¡æ‰«ææœªå®Œæ•´å®Œæˆï¼Œç»“æœå¯èƒ½ä¸å…¨ã€‚")
    elif cached.get("error"):
        st.info(f"ä¸Šæ¬¡æ‰«æå¤±è´¥: {cached['error'].splitlines()[0]}")

    results = cached.get("results", [])
    if not results:
        st.info("ä¸Šæ¬¡æ‰«ææ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨ã€‚")
        return

    df = pd.DataFrame(results)
    if not df.empty:
        df = df.rename(
            columns={
                "code": "ä»£ç ",
                "name": "åç§°",
                "industry": "æ‰€å±è¡Œä¸š",
            }
        )
        display_df = df.head(MAX_PREVIOUS_RESULTS)
        st.table(display_df)
        if len(df) > MAX_PREVIOUS_RESULTS:
            st.caption(f"ä»…æ˜¾ç¤ºå‰ {MAX_PREVIOUS_RESULTS} æ¡ï¼Œå…± {len(df)} æ¡ã€‚")


def render_previous_scan_section():
    """Show cards for both scan modes on the main page."""
    st.subheader("å†å²æ‰«æç»“æœ")
    col1, col2 = st.columns(2)
    with col1:
        render_previous_scan_card("æŒ‰è¡Œä¸šæ¿å—")
    with col2:
        render_previous_scan_card("å…¨éƒ¨ A è‚¡")


# Add a caching mechanism for expensive API calls with local file support
@st.cache_data(ttl=60)  # Cache data for 1 minute in Streamlit's cache
def fetch_industry_data():
    """Fetch and cache all industry data, using local file when possible"""
    try:
        # Define directory for cache files
        cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
        os.makedirs(cache_dir, exist_ok=True)

        # Find the most recent industry cache file
        cache_files = [
            f
            for f in os.listdir(cache_dir)
            if f.startswith("industry_data_") and f.endswith(".json")
        ]
        latest_file = None
        is_cache_valid = False

        if cache_files:
            # Get the most recent file
            cache_files.sort(reverse=True)  # Sort by filename (which includes date)
            latest_file = os.path.join(cache_dir, cache_files[0])

            # Extract date from filename (industry_data_YYYY-MM-DD.json)
            try:
                file_date_str = (
                    cache_files[0].replace("industry_data_", "").replace(".json", "")
                )
                file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                # Check if file is less than 2 months old
                is_cache_valid = (datetime.now() - file_date).days < 60
            except:
                is_cache_valid = False

        # Load from cache file if valid
        if is_cache_valid and latest_file and os.path.exists(latest_file):
            progress_text = st.empty()
            progress_text.text("ä»æœ¬åœ°ç¼“å­˜åŠ è½½è¡Œä¸šæ•°æ®...")

            with open(latest_file, "r", encoding="utf-8") as f:
                cached_data = json.load(f)

            progress_text.empty()
            cached_data.setdefault("industry_change_pct", {})
            return cached_data

        # If cache is invalid or doesn't exist, fetch fresh data
        progress_text = st.empty()
        progress_text.text("æ­£åœ¨ä» Tushare è·å–è¡Œä¸šæ•°æ®...")

        industry_data = build_industry_payload()

        # Save to a new cache file with current date
        current_date = datetime.now().strftime("%Y-%m-%d")
        cache_file = os.path.join(cache_dir, f"industry_data_{current_date}.json")

        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(industry_data, f, ensure_ascii=False, indent=2)

        # Clean up old cache files (keep only the most recent 3)
        cache_files = [
            f
            for f in os.listdir(cache_dir)
            if f.startswith("industry_data_") and f.endswith(".json")
        ]
        if len(cache_files) > 3:
            cache_files.sort()  # Sort by date ascending
            for old_file in cache_files[:-3]:  # Remove all but the 3 most recent
                try:
                    os.remove(os.path.join(cache_dir, old_file))
                except:
                    pass

        progress_text.empty()
        return industry_data

    except Exception as e:
        st.error(f"è·å–è¡Œä¸šæ•°æ®å¤±è´¥: {str(e)}")
        return {
            "industry_list": [],
            "industry_stocks": {},
            "industry_counts": {},
            "stock_to_industry": {},
            "industry_change_pct": {},
            "fetch_date": datetime.now().strftime("%Y-%m-%d"),
        }


# Sidebar options
st.sidebar.title("åˆ†ææ¨¡å¼")
analysis_mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["è‡ªåŠ¨æ‰«æä¹°å…¥ä¿¡å·", "å•ä¸€è‚¡ç¥¨åˆ†æ"])

if analysis_mode == "å•ä¸€è‚¡ç¥¨åˆ†æ":
    # Single stock analysis mode - similar to the original code
    st.sidebar.title("è‚¡ç¥¨è¾“å…¥")
    ticker = st.sidebar.text_input("è¾“å…¥ 6 ä½è‚¡ç¥¨ä»£ç ï¼ˆä¾‹å¦‚ï¼Œ000001ï¼‰", "000001")

    st.sidebar.title("æ˜¾ç¤ºé€‰é¡¹")
    show_short_term = st.sidebar.checkbox("æ˜¾ç¤ºçŸ­æœŸ EMA", value=True)
    show_long_term = st.sidebar.checkbox("æ˜¾ç¤ºé•¿æœŸ EMA", value=True)

    # Calculate date range for the past 6 months
    end_date = datetime.today().strftime("%Y%m%d")
    start_date = (datetime.today() - timedelta(days=180)).strftime("%Y%m%d")

    # Fetch and process stock data
    with st.spinner("è·å–æ•°æ®ä¸­..."):
        try:
            # Remove exchange suffix if present (e.g., '000001.SZ' -> '000001')
            ticker = ticker.split(".")[0]
            if not ticker.isdigit() or len(ticker) != 6:
                st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„ 6 ä½è‚¡ç¥¨ä»£ç ã€‚")
            else:
                ticker = normalize_symbol(ticker)
                stock_data = fetch_daily_history(ticker, start_date, end_date)
                if stock_data.empty:
                    st.error("æœªæ‰¾åˆ°æ‰€è¾“å…¥è‚¡ç¥¨ä»£ç çš„æ•°æ®ã€‚è¯·æ£€æŸ¥ä»£ç å¹¶é‡è¯•ã€‚")
                else:
                    # Calculate Exponential Moving Averages (EMAs)
                    for period in [3, 5, 8, 10, 12, 15, 30, 35, 40, 45, 50, 60]:
                        stock_data[f"EMA{period}"] = (
                            stock_data["close"].ewm(span=period, adjust=False).mean()
                        )

                    # Define short-term and long-term EMAs
                    short_terms = [3, 5, 8, 10, 12, 15]
                    long_terms = [30, 35, 40, 45, 50, 60]

                    # Calculate average of short-term and long-term EMAs for each day
                    stock_data["avg_short_ema"] = stock_data[
                        [f"EMA{period}" for period in short_terms]
                    ].mean(axis=1)
                    stock_data["avg_long_ema"] = stock_data[
                        [f"EMA{period}" for period in long_terms]
                    ].mean(axis=1)

                    # Detect crossovers (short-term crossing above long-term)
                    stock_data["short_above_long"] = (
                        stock_data["avg_short_ema"] > stock_data["avg_long_ema"]
                    )
                    stock_data["crossover"] = False

                    # Find the exact crossover points (when short_above_long changes from False to True)
                    for i in range(1, len(stock_data)):
                        if (
                            not stock_data["short_above_long"].iloc[i - 1]
                            and stock_data["short_above_long"].iloc[i]
                        ):
                            # Replace: stock_data['crossover'].iloc[i] = True
                            stock_data.loc[stock_data.index[i], "crossover"] = True

                    # Create Plotly figure
                    fig = go.Figure()
                    high_series = (
                        stock_data["high"]
                        if "high" in stock_data
                        else stock_data[["open", "close"]].max(axis=1)
                    )
                    low_series = (
                        stock_data["low"]
                        if "low" in stock_data
                        else stock_data[["open", "close"]].min(axis=1)
                    )

                    # Add candlestick chart
                    fig.add_trace(
                        go.Candlestick(
                            x=stock_data.index,
                            open=stock_data["open"],
                            high=high_series,
                            low=low_series,
                            close=stock_data["close"],
                            increasing_line_color="green",
                            decreasing_line_color="red",
                            name="Price",
                        )
                    )

                    # Add short-term EMAs (blue)
                    if show_short_term:
                        for i, period in enumerate([3, 5, 8, 10, 12, 15]):
                            fig.add_trace(
                                go.Scatter(
                                    x=stock_data.index,
                                    y=stock_data[f"EMA{period}"],
                                    mode="lines",
                                    name=f"EMA{period}",
                                    line=dict(color="blue", width=1),
                                    legendgroup="short_term",
                                    showlegend=(i == 0),
                                )
                            )

                    # Add long-term EMAs (red)
                    if show_long_term:
                        for i, period in enumerate([30, 35, 40, 45, 50, 60]):
                            fig.add_trace(
                                go.Scatter(
                                    x=stock_data.index,
                                    y=stock_data[f"EMA{period}"],
                                    mode="lines",
                                    name=f"EMA{period}",
                                    line=dict(color="red", width=1),
                                    legendgroup="long_term",
                                    showlegend=(i == 0),
                                )
                            )

                    # Add average short-term and long-term EMAs to visualize crossover
                    fig.add_trace(
                        go.Scatter(
                            x=stock_data.index,
                            y=stock_data["avg_short_ema"],
                            mode="lines",
                            name="Avg Short-term EMAs",
                            line=dict(color="blue", width=2, dash="dot"),
                        )
                    )

                    fig.add_trace(
                        go.Scatter(
                            x=stock_data.index,
                            y=stock_data["avg_long_ema"],
                            mode="lines",
                            name="Avg Long-term EMAs",
                            line=dict(color="red", width=2, dash="dot"),
                        )
                    )

                    # Mark crossover signals on the chart
                    crossover_dates = stock_data[stock_data["crossover"]].index
                    for date in crossover_dates:
                        price_at_crossover = stock_data.loc[date, "close"]
                        # Add vertical line at crossover
                        fig.add_shape(
                            type="line",
                            x0=date,
                            y0=price_at_crossover * 0.97,
                            x1=date,
                            y1=price_at_crossover * 1.03,
                            line=dict(color="green", width=3),
                        )
                        # Add annotation
                        fig.add_annotation(
                            x=date,
                            y=price_at_crossover * 1.04,
                            text="ä¹°å…¥ä¿¡å·",
                            showarrow=True,
                            arrowhead=1,
                            arrowcolor="green",
                            arrowsize=1,
                            arrowwidth=2,
                            font=dict(color="green", size=12),
                        )

                    # Count and display the number of signals
                    signal_count = len(crossover_dates)
                    if signal_count > 0:
                        last_signal = (
                            crossover_dates[-1].strftime("%Y-%m-%d")
                            if signal_count > 0
                            else "None"
                        )
                        signal_info = f"**ä¹°å…¥ä¿¡å·**: å…± {signal_count} ä¸ª, æœ€è¿‘ä¿¡å·æ—¥æœŸ: {last_signal}"
                        fig.add_annotation(
                            x=0.02,
                            y=0.98,
                            xref="paper",
                            yref="paper",
                            text=signal_info,
                            showarrow=False,
                            font=dict(size=14, color="green"),
                            bgcolor="white",
                            bordercolor="green",
                            borderwidth=1,
                            align="left",
                        )

                    # Customize plot layout
                    fig.update_layout(
                        title=f"è‚¡ç¥¨ {ticker} GMMA å›¾è¡¨ (æ ‡è®°: çŸ­æœŸEMAä»ä¸‹æ–¹ç©¿è¿‡é•¿æœŸEMA)",
                        xaxis_title="æ—¥æœŸ",
                        yaxis_title="ä»·æ ¼",
                        legend_title="å›¾ä¾‹",
                        hovermode="x unified",
                        template="plotly_white",
                    )

                    # Display the plot in Streamlit
                    st.plotly_chart(fig, use_container_width=True)

                    # Display crossover days in a table
                    if len(crossover_dates) > 0:
                        st.subheader("ä¹°å…¥ä¿¡å·æ—¥æœŸ")
                        signal_df = pd.DataFrame(crossover_dates, columns=["æ—¥æœŸ"])
                        signal_df["æ—¥æœŸ"] = signal_df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")
                        st.table(signal_df)
        except Exception as e:
            st.error(f"è·å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")

else:  # Auto scan mode
    st.sidebar.title("æ‰«æè®¾ç½®")
    days_to_check = st.sidebar.slider("æ£€æŸ¥æœ€è¿‘å‡ å¤©å†…çš„ä¿¡å·", 1, 7, 1)
    max_stocks = st.sidebar.slider("æœ€å¤šæ˜¾ç¤ºè‚¡ç¥¨æ•°é‡", 1, 200, 200)

    # Add industry selection option
    scan_mode = st.sidebar.radio("æ‰«æèŒƒå›´", ["æŒ‰è¡Œä¸šæ¿å—", "å…¨éƒ¨ A è‚¡"])

    # Add filtering options back with different default values
    if st.sidebar.checkbox("æ˜¾ç¤ºé«˜çº§è¿‡æ»¤é€‰é¡¹", value=False):
        st.sidebar.subheader("è¿‡æ»¤é€‰é¡¹")
        exclude_st = st.sidebar.checkbox("æ’é™¤STè‚¡ç¥¨", value=True)
        exclude_688 = st.sidebar.checkbox("æ’é™¤ç§‘åˆ›æ¿è‚¡ç¥¨ (688å¼€å¤´)", value=True)
        exclude_300 = st.sidebar.checkbox("æ’é™¤åˆ›ä¸šæ¿è‚¡ç¥¨ (300å¼€å¤´)", value=True)
        exclude_8 = st.sidebar.checkbox("æ’é™¤åŒ—äº¤æ‰€è‚¡ç¥¨ (8å¼€å¤´)", value=True)
        exclude_4 = st.sidebar.checkbox("æ’é™¤ä¸‰æ¿è‚¡ç¥¨ (4å¼€å¤´)", value=True)
    else:
        # Default filtering values
        exclude_st = True
        exclude_688 = True if scan_mode == "å…¨éƒ¨ A è‚¡" else False
        exclude_300 = True if scan_mode == "å…¨éƒ¨ A è‚¡" else False
        exclude_8 = True if scan_mode == "å…¨éƒ¨ A è‚¡" else False
        exclude_4 = True if scan_mode == "å…¨éƒ¨ A è‚¡" else False

    selected_industry = None
    selected_industries = []  # Initialize with empty list to prevent NameError
    industry_data = None

    # Industry board selection
    if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
        try:
            # Fetch all industry data once (cached)
            with st.spinner("è·å–è¡Œä¸šæ¿å—æ•°æ®..."):
                industry_data = fetch_industry_data()
                industry_list = industry_data["industry_list"]
                industry_counts = industry_data["industry_counts"]
                industry_stocks = industry_data["industry_stocks"]

                industry_change_pct = industry_data.get("industry_change_pct", {})

            # Remove loading message from sidebar
            st.sidebar.text("")

            # Format options to include stock count and æ¶¨è·Œå¹…: "è¡Œä¸šå (123è‚¡) â†‘2.50%"
            industry_options = []
            for ind in industry_list:
                count = industry_counts[ind]
                # Get the latest change percentage for this industry
                change_pct = industry_change_pct.get(ind, 0)

                # Format percentage with arrow indicator and color
                if change_pct > 0:
                    pct_str = f"â†‘{change_pct:.2f}%"
                elif change_pct < 0:
                    pct_str = f"â†“{abs(change_pct):.2f}%"
                else:
                    pct_str = f"0.00%"

                # Create the formatted option
                option = f"{ind} ({count}è‚¡) {pct_str}"
                industry_options.append(option)

            # Create a mapping from formatted name back to original name
            industry_name_mapping = {
                option: ind for ind, option in zip(industry_list, industry_options)
            }

            # Sort industries by intraday performance (descending)
            industry_options_sorted = sorted(
                industry_options,
                key=lambda x: industry_change_pct.get(
                    industry_name_mapping[x], float("-inf")
                ),
                reverse=True,
            )

            # Use the sorted options in the multiselect
            default_option = (
                industry_options_sorted[0] if industry_options_sorted else None
            )
            selected_industry_options = st.sidebar.multiselect(
                "é€‰æ‹©è¡Œä¸šæ¿å— (å¯å¤šé€‰)",
                options=industry_options_sorted,
                default=[default_option] if default_option else [],
            )

            # Convert the selected formatted options back to original industry names
            selected_industries = [
                industry_name_mapping[opt] for opt in selected_industry_options
            ]

            # Show the selected industry info
            if selected_industries:
                total_stocks = sum(industry_counts[ind] for ind in selected_industries)
                industries_text = ", ".join(selected_industries)
                st.sidebar.info(
                    f"å·²é€‰æ‹©: {industries_text}\n\nå…±è®¡çº¦ {total_stocks} åªè‚¡ç¥¨"
                )
        except Exception as e:
            st.sidebar.error(f"è·å–è¡Œä¸šæ¿å—å¤±è´¥: {str(e)}")
    else:
        industry_data = fetch_industry_data()

    if industry_data is None:
        industry_data = fetch_industry_data()

    start_scan_clicked = st.sidebar.button("å¼€å§‹æ‰«æ")

    results_tab, history_tab = st.tabs(["æ‰«æç»“æœ", "å†å²æ‰«æè®°å½•"])

    with history_tab:
        render_previous_scan_section()

    with results_tab:
        if start_scan_clicked:
            crossover_stocks = []
            stock_errors = []
            progress_bar = None
            scan_partial = False
            scan_error_message = None
            with st.spinner("æ­£åœ¨æ‰«æä¹°å…¥ä¿¡å·ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´..."):
                try:
                    # Variable to track if we have valid stocks to scan
                    have_stocks_to_scan = True

                    # Get stock list based on scan mode
                    if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" and selected_industries:
                        # Create an empty DataFrame to store all industry stocks
                        all_industry_stocks_list = []

                        # Use cached data instead of making new API calls
                        for industry in selected_industries:
                            if industry in industry_stocks:
                                all_industry_stocks_list.extend(
                                    industry_stocks[industry]
                                )

                        if all_industry_stocks_list:
                            # Convert to DataFrame
                            stock_info_df = pd.DataFrame(
                                all_industry_stocks_list, columns=["code", "name"]
                            )
                            # Remove duplicates
                            stock_info_df = stock_info_df.drop_duplicates(
                                subset=["code"]
                            )
                        else:
                            st.error("æœªèƒ½è·å–æ‰€é€‰è¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨ã€‚")
                            have_stocks_to_scan = False
                    else:
                        # Get all A-share stock codes and names
                        stock_info_df = load_stock_basic()[["code", "name"]].copy()

                    # Only proceed if we have stocks to scan
                    if have_stocks_to_scan:
                        # Show how many stocks will be scanned
                        stock_count = len(stock_info_df)
                        st.info(f"å‡†å¤‡æ‰«æ {stock_count} åªè‚¡ç¥¨...")

                        # Create a progress bar
                        progress_bar = st.progress(0)

                        # Create industry mapping dictionary for multiple industry case
                        industry_mapping = {}

                        # If in industry mode, map stock codes to their industries
                        if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
                            # Create industry mapping from the cached data
                            industry_mapping = industry_data["stock_to_industry"]

                        # Loop through selected stocks
                        for i, row in enumerate(stock_info_df.itertuples()):
                            # Update progress
                            progress_bar.progress(min((i + 1) / stock_count, 1.0))

                            ticker = row.code
                            name = row.name

                            # Skip stocks with special prefixes only if scanning all stocks
                            if scan_mode == "å…¨éƒ¨ A è‚¡" and ticker.startswith(
                                ("688", "300", "8", "4")
                            ):
                                continue

                            # Skip stocks based on filter settings
                            if (
                                (exclude_688 and ticker.startswith("688"))
                                or (exclude_300 and ticker.startswith("300"))
                                or (exclude_8 and ticker.startswith("8"))
                                or (exclude_4 and ticker.startswith("4"))
                            ):
                                continue

                            try:
                                # Check for crossover
                                has_crossover, stock_data = has_recent_crossover(
                                    ticker, days_to_check
                                )

                                if not has_crossover:
                                    continue

                                # Get industry information for the stock
                                if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
                                    # Use the mapped industry from the cached data
                                    industry = industry_mapping.get(ticker, "æœªçŸ¥")
                                else:
                                    industry = industry_data["stock_to_industry"].get(
                                        ticker, "æœªçŸ¥"
                                    )

                                # Include industry in the crossover_stocks list
                                crossover_stocks.append(
                                    (ticker, name, industry, stock_data)
                                )

                                # Create tab for this stock
                                with st.expander(
                                    f"{ticker} - {name} ({industry})", expanded=True
                                ):
                                    # Create GMMA chart
                                    fig = go.Figure()
                                    high_series = (
                                        stock_data["high"]
                                        if "high" in stock_data
                                        else stock_data[["open", "close"]].max(axis=1)
                                    )
                                    low_series = (
                                        stock_data["low"]
                                        if "low" in stock_data
                                        else stock_data[["open", "close"]].min(axis=1)
                                    )

                                    # Add candlestick chart
                                    fig.add_trace(
                                        go.Candlestick(
                                            x=stock_data.index,
                                            open=stock_data["open"],
                                            high=high_series,
                                            low=low_series,
                                            close=stock_data["close"],
                                            increasing_line_color="red",
                                            decreasing_line_color="green",
                                            name="Price",
                                        )
                                    )

                                    # Add short-term EMAs (blue)
                                    for i, period in enumerate([3, 5, 8, 10, 12, 15]):
                                        fig.add_trace(
                                            go.Scatter(
                                                x=stock_data.index,
                                                y=stock_data[f"EMA{period}"],
                                                mode="lines",
                                                name=f"EMA{period}",
                                                line=dict(color="blue", width=1),
                                                legendgroup="short_term",
                                                showlegend=(i == 0),
                                            )
                                        )

                                    # Add long-term EMAs (red)
                                    for i, period in enumerate([30, 35, 40, 45, 50, 60]):
                                        fig.add_trace(
                                            go.Scatter(
                                                x=stock_data.index,
                                                y=stock_data[f"EMA{period}"],
                                                mode="lines",
                                                name=f"EMA{period}",
                                                line=dict(color="red", width=1),
                                                legendgroup="long_term",
                                                showlegend=(i == 0),
                                            )
                                        )

                                    # Add average EMAs
                                    fig.add_trace(
                                        go.Scatter(
                                            x=stock_data.index,
                                            y=stock_data["avg_short_ema"],
                                            mode="lines",
                                            name="Avg Short-term EMAs",
                                            line=dict(color="blue", width=2, dash="dot"),
                                        )
                                    )

                                    fig.add_trace(
                                        go.Scatter(
                                            x=stock_data.index,
                                            y=stock_data["avg_long_ema"],
                                            mode="lines",
                                            name="Avg Long-term EMAs",
                                            line=dict(color="red", width=2, dash="dot"),
                                        )
                                    )

                                    # Mark crossover signals
                                    crossover_dates = stock_data[
                                        stock_data["crossover"]
                                    ].index
                                    for date in crossover_dates:
                                        price_at_crossover = stock_data.loc[date, "close"]
                                        fig.add_annotation(
                                            x=date,
                                            y=price_at_crossover * 1.04,
                                            text="ä¹°å…¥ä¿¡å·",
                                            showarrow=True,
                                            arrowhead=1,
                                            arrowcolor="green",
                                            arrowsize=1,
                                            arrowwidth=2,
                                            font=dict(color="green", size=12),
                                        )

                                    # Layout
                                    fig.update_layout(
                                        title=f"{ticker} - {name} GMMA å›¾è¡¨",
                                        xaxis_title="æ—¥æœŸ",
                                        yaxis_title="ä»·æ ¼",
                                        legend_title="å›¾ä¾‹",
                                        hovermode="x unified",
                                        template="plotly_white",
                                        height=500,
                                    )

                                    # Display the plot
                                    st.plotly_chart(fig, use_container_width=True)
                            except Exception as stock_exc:
                                stock_errors.append((ticker, str(stock_exc)))
                                continue

                            # Check if we found enough stocks
                            if len(crossover_stocks) >= max_stocks:
                                break

                        # Final update
                        progress_bar.progress(1.0)

                        display_scan_results(
                            crossover_stocks, scan_mode, selected_industries, days_to_check
                        )

                        if stock_errors:
                            example_errors = "; ".join(
                                f"{code}: {msg.splitlines()[0]}"
                                for code, msg in stock_errors[:3]
                            )
                            st.warning(
                                f"{len(stock_errors)} åªè‚¡ç¥¨æ‰«æå¤±è´¥ï¼Œç¤ºä¾‹: {example_errors}"
                            )

                except Exception as e:
                    scan_error_message = str(e)
                    if progress_bar is not None:
                        progress_bar.progress(1.0)

                    if crossover_stocks:
                        scan_partial = True
                        st.warning(f"æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}ã€‚ä»¥ä¸‹ä¸ºéƒ¨åˆ†ç»“æœã€‚")
                        display_scan_results(
                            crossover_stocks,
                            scan_mode,
                            selected_industries,
                            days_to_check,
                            partial=True,
                        )
                    else:
                        st.error(f"æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")

                    if stock_errors:
                        example_errors = "; ".join(
                            f"{code}: {msg.splitlines()[0]}"
                            for code, msg in stock_errors[:3]
                        )
                        st.warning(
                            f"{len(stock_errors)} åªè‚¡ç¥¨æ‰«æå¤±è´¥ï¼Œç¤ºä¾‹: {example_errors}"
                        )
                finally:
                    save_scan_results_to_cache(
                        scan_mode,
                        selected_industries if scan_mode == "æŒ‰è¡Œä¸šæ¿å—" else [],
                        days_to_check,
                        crossover_stocks,
                        partial=scan_partial,
                        error_message=scan_error_message,
                    )
        else:
            if scan_mode == "æŒ‰è¡Œä¸šæ¿å—":
                if selected_industries:
                    industry_count = len(selected_industries)
                    total_stocks = sum(
                        industry_counts.get(ind, 0) for ind in selected_industries
                    )
                    industries_text = (
                        f"{industry_count} ä¸ªè¡Œä¸š (çº¦ {total_stocks} åªè‚¡ç¥¨)"
                        if industry_count > 1
                        else f"{selected_industries[0]} (çº¦ {industry_counts.get(selected_industries[0], 0)} åªè‚¡ç¥¨)"
                    )
                    st.info(
                        f"è¯·ç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ä»¥æŸ¥æ‰¾ {industries_text} ä¸­æœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚"
                    )
                else:
                    st.info("è¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€ä¸ªè¡Œä¸šæ¿å—ï¼Œç„¶åç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ã€‚")
            else:
                st.info("è¯·ç‚¹å‡»'å¼€å§‹æ‰«æ'æŒ‰é’®ä»¥æŸ¥æ‰¾æœ€è¿‘å‡ºç°ä¹°å…¥ä¿¡å·çš„è‚¡ç¥¨ã€‚")
