import os
import random
import time
import traceback
from collections.abc import Mapping
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from threading import Lock

import numpy as np
import pandas as pd
import streamlit as st
import tushare as ts

# Set page layout to wide mode
st.set_page_config(
    page_title="Aè‚¡å¸‚å€¼å˜åŒ–æ’åºå™¨",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# App title and description
st.title("Aè‚¡å¸‚å€¼å˜åŒ–æ’åºå·¥å…·")
st.markdown("""
æ­¤åº”ç”¨ç¨‹åºä½¿ç”¨ Tushare æ•°æ®åˆ†æä¸­å›½ A è‚¡è‚¡ç¥¨åœ¨æŒ‡å®šæ—¶é—´åŒºé—´å†…çš„æ€»å¸‚å€¼å˜åŒ–ã€‚
å®ƒä¼šåŸºäºç”¨æˆ·é€‰æ‹©çš„èµ·æ­¢æ—¥æœŸå¹¶å‘æ‰«æå…¨å¸‚åœºï¼Œåˆ—å‡ºæ€»å¸‚å€¼å¢å¹…æœ€å¤§çš„å‰50åªè‚¡ç¥¨ä»¥åŠè·Œå¹…æœ€å¤§çš„å‰50åªè‚¡ç¥¨ã€‚
""")

# Create cache directory if it doesn't exist
cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
os.makedirs(cache_dir, exist_ok=True)

MAX_WORKERS = min(6, max(2, (os.cpu_count() or 2)))
DEFAULT_LOOKBACK_DAYS = 30
TOP_COUNT = 50
ERROR_LOG_DISPLAY_LIMIT = 100
ERROR_LOG_FILE = os.path.join(cache_dir, "value_sorting_errors.log")
STOCK_LIST_CACHE_FILE = os.path.join(cache_dir, "stock_list.csv")
STOCK_LIST_REFRESH_DAYS = 7
REQUEST_MAX_RETRIES = 3
REQUEST_MIN_INTERVAL = 0.08  # seconds between outbound requests
REQUEST_JITTER = 0.03        # random jitter to avoid fixed pattern
REQUEST_BACKOFF_FACTOR = 1.8
REQUEST_BACKOFF_MAX = 6.0
RATE_LIMIT_MSG = "æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®"

_request_lock = Lock()
_next_available_time = 0.0


def _get_tushare_token_from_secrets() -> str | None:
    """ä» Streamlit secrets ä¸­è¯»å– Tushare æˆæƒä»¤ç‰Œã€‚"""
    secret_token = None
    tushare_section = st.secrets.get("tushare")
    if tushare_section and isinstance(tushare_section, Mapping):
        secret_token = (
            tushare_section.get("token")
            or tushare_section.get("api_token")
            or tushare_section.get("TUSHARE_TOKEN")
        )
    return secret_token or st.secrets.get("tushare_token") or st.secrets.get(
        "TUSHARE_TOKEN"
    )


TUSHARE_TOKEN = (
    _get_tushare_token_from_secrets()
    or os.getenv("TUSHARE_TOKEN")
    or os.getenv("TS_TOKEN")
    or os.getenv("TUSHARE_PRO_TOKEN")
)


def to_ts_code(code: str) -> str:
    """å°† 6 ä½è‚¡ç¥¨ä»£ç è½¬æ¢ä¸º Tushare ts_code æ ¼å¼ã€‚"""
    code = str(code).zfill(6)
    if code.startswith(("6", "9")) or code.startswith(("688", "689")):
        return f"{code}.SH"
    if code.startswith(("4", "8")):
        return f"{code}.BJ"
    return f"{code}.SZ"


@st.cache_resource
def get_tushare_client():
    """ç¼“å­˜å¹¶è¿”å› Tushare Pro å®¢æˆ·ç«¯å®ä¾‹ã€‚"""
    if not TUSHARE_TOKEN:
        raise RuntimeError("è¯·åœ¨ TUSHARE_TOKEN/TS_TOKEN ç¯å¢ƒå˜é‡æˆ– st.secrets ä¸­é…ç½® Tushare æˆæƒä»¤ç‰Œã€‚")
    return ts.pro_api(TUSHARE_TOKEN)


def call_tushare_api(func, *, api_label: str):
    """åŒ…è£… Tushare API è°ƒç”¨å¹¶æä¾›é€€é¿ä¸é”™è¯¯æ—¥å¿—ã€‚"""
    last_exception = None
    for attempt in range(1, REQUEST_MAX_RETRIES + 1):
        acquire_request_slot()
        try:
            return func()
        except Exception as exc:
            last_exception = exc
            message = str(exc)
            if attempt == REQUEST_MAX_RETRIES:
                break
            delay = backoff_delay(attempt)
            if RATE_LIMIT_MSG in message:
                append_error_log(
                    f"Tushare é™é¢‘: {api_label} ç¬¬ {attempt} æ¬¡è°ƒç”¨å¤±è´¥ï¼Œå°†åœ¨ {delay:.2f}s åé‡è¯•ã€‚é”™è¯¯: {message}"
                )
            else:
                append_error_log(
                    f"Tushare API {api_label} ç¬¬ {attempt} æ¬¡è°ƒç”¨å¤±è´¥ï¼Œå°†åœ¨ {delay:.2f}s åé‡è¯•ã€‚é”™è¯¯: {message}"
                )
            time.sleep(delay)
    if last_exception:
        raise last_exception
    return None


def prepare_stock_list_df(df: pd.DataFrame | None) -> pd.DataFrame:
    """æ ‡å‡†åŒ–è‚¡ç¥¨åˆ—è¡¨æ•°æ®ç»“æ„ï¼Œç¡®ä¿åŒ…å« code/name/ts_code åˆ—ã€‚"""
    if df is None or df.empty:
        return pd.DataFrame(columns=['code', 'name', 'ts_code'])
    normalised = df.copy()
    if 'code' in normalised.columns:
        normalised['code'] = (
            normalised['code']
            .astype(str)
            .str.replace(r"\.0$", "", regex=True)
            .str.zfill(6)
        )
    elif 'symbol' in normalised.columns:
        normalised['code'] = (
            normalised['symbol']
            .astype(str)
            .str.replace(r"\.0$", "", regex=True)
            .str.zfill(6)
        )
    else:
        normalised['code'] = normalised.index.astype(str).str.zfill(6)
    if 'name' not in normalised.columns:
        normalised['name'] = ""
    normalised['name'] = normalised['name'].fillna("").astype(str)
    if 'ts_code' not in normalised.columns:
        normalised['ts_code'] = normalised['code'].apply(to_ts_code)
    else:
        normalised['ts_code'] = normalised['ts_code'].fillna("").astype(str)
        missing_mask = normalised['ts_code'].eq("")
        normalised.loc[missing_mask, 'ts_code'] = normalised.loc[
            missing_mask, 'code'
        ].apply(to_ts_code)
    return normalised


# Cache for stock list to avoid repeated API calls
@st.cache_data(ttl=3600)  # Cache for 1 hour
def get_stock_list():
    """è·å–æ‰€æœ‰æ²ªæ·±Aè‚¡çš„ä»£ç å’Œåç§°ï¼Œå¹¶åœ¨æœ¬åœ°ç¼“å­˜ä¸ƒå¤©"""
    try:
        use_cached_file = False
        if os.path.exists(STOCK_LIST_CACHE_FILE):
            file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(STOCK_LIST_CACHE_FILE))
            if file_age <= timedelta(days=STOCK_LIST_REFRESH_DAYS):
                use_cached_file = True
        if use_cached_file:
            cached_df = pd.read_csv(STOCK_LIST_CACHE_FILE)
            return prepare_stock_list_df(cached_df)

        client = get_tushare_client()
        fields = "ts_code,symbol,name,market,list_date"
        stock_list_df = call_tushare_api(
            lambda: client.stock_basic(exchange="", list_status="L", fields=fields),
            api_label="stock_basic",
        )
        if stock_list_df is not None and not stock_list_df.empty:
            stock_list_df = stock_list_df.rename(columns={"symbol": "code"})
            stock_list_df = prepare_stock_list_df(stock_list_df)
            try:
                stock_list_df.to_csv(STOCK_LIST_CACHE_FILE, index=False, encoding="utf-8")
            except Exception as write_err:
                append_error_log(f"ä¿å­˜è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å¤±è´¥: {write_err}")
        elif os.path.exists(STOCK_LIST_CACHE_FILE):
            cached_df = pd.read_csv(STOCK_LIST_CACHE_FILE)
            return prepare_stock_list_df(cached_df)
        return stock_list_df or pd.DataFrame(columns=['code', 'name', 'ts_code'])
    except RuntimeError as token_err:
        st.error(str(token_err))
        return pd.DataFrame(columns=['code', 'name', 'ts_code'])
    except Exception as e:
        append_error_log(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        if os.path.exists(STOCK_LIST_CACHE_FILE):
            try:
                cached_df = pd.read_csv(STOCK_LIST_CACHE_FILE)
                return prepare_stock_list_df(cached_df)
            except Exception as read_err:
                append_error_log(f"è¯»å–æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å¤±è´¥: {read_err}")
        st.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return pd.DataFrame(columns=['code', 'name', 'ts_code'])

def append_error_log(message):
    """å°†é”™è¯¯ä¿¡æ¯å†™å…¥æ—¥å¿—æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(ERROR_LOG_FILE, "a", encoding="utf-8") as log_file:
            log_file.write(f"[{timestamp}] {message}\n")
    except Exception:
        pass
    
def acquire_request_slot():
    """Rate limit outbound requests toé¿å…è§¦å‘è¿œç«¯é™æµ"""
    global _next_available_time
    with _request_lock:
        now = time.monotonic()
        wait = _next_available_time - now
        if wait < 0:
            wait = 0.0
        _next_available_time = now + wait + REQUEST_MIN_INTERVAL
    if wait > 0:
        time.sleep(wait)
    if REQUEST_JITTER > 0:
        time.sleep(random.uniform(0, REQUEST_JITTER))
    
def backoff_delay(attempt):
    """Calculate exponential backoff delay for retries"""
    delay = REQUEST_BACKOFF_FACTOR ** attempt
    return min(delay, REQUEST_BACKOFF_MAX)

@st.cache_data(ttl=900, show_spinner=False)
def get_stock_market_value(symbol, start_date, end_date, *, ts_code=None, stock_name=None):
    """è·å–æŒ‡å®šè‚¡ç¥¨åœ¨ç»™å®šæ—¥æœŸèŒƒå›´çš„ Tushare æ€»å¸‚å€¼ä¸æµé€šå¸‚å€¼æ•°æ®ã€‚"""
    symbol_no_prefix = str(symbol).zfill(6)
    ts_code = ts_code or to_ts_code(symbol_no_prefix)
    display_name = stock_name or symbol_no_prefix
    try:
        client = get_tushare_client()
    except RuntimeError as token_err:
        append_error_log(f"åˆå§‹åŒ– Tushare å®¢æˆ·ç«¯å¤±è´¥: {token_err}")
        return None
    fields = (
        "ts_code,trade_date,close,total_mv,circ_mv,total_share,float_share,free_share"
    )
    try:
        hist_df = call_tushare_api(
            lambda: client.daily_basic(
                ts_code=ts_code,
                start_date=start_date,
                end_date=end_date,
                fields=fields,
            ),
            api_label=f"daily_basic:{ts_code}",
        )
    except Exception as exc:
        append_error_log(f"è·å– {symbol_no_prefix} Tushare æ•°æ®å¤±è´¥: {exc}")
        return None

    if hist_df is None or hist_df.empty:
        return None

    hist_df = hist_df.copy()
    if 'trade_date' not in hist_df.columns:
        append_error_log(f"{ts_code} è¿”å›æ•°æ®ä¸­ç¼ºå°‘ trade_date åˆ—")
        return None
    numeric_cols = [
        'close',
        'total_mv',
        'circ_mv',
        'total_share',
        'float_share',
        'free_share',
    ]
    for col in numeric_cols:
        if col in hist_df.columns:
            hist_df[col] = pd.to_numeric(hist_df[col], errors='coerce')
    hist_df['æ—¥æœŸ'] = pd.to_datetime(hist_df['trade_date'], format='%Y%m%d', errors='coerce')
    hist_df = hist_df.dropna(subset=['æ—¥æœŸ']).sort_values('æ—¥æœŸ').reset_index(drop=True)
    if hist_df.empty:
        return None

    hist_df['åç§°'] = display_name
    if 'close' in hist_df.columns:
        hist_df['æ”¶ç›˜'] = hist_df['close']

    # Convert value units (Tushare total/circ mv are in 1e4 RMB)
    if 'total_mv' in hist_df.columns:
        hist_df['æ€»å¸‚å€¼'] = hist_df['total_mv'] * 1e4
    if 'circ_mv' in hist_df.columns:
        hist_df['æµé€šå¸‚å€¼'] = hist_df['circ_mv'] * 1e4

    # Convert share counts (Tushare share columns use ä¸‡è‚¡å•ä½)
    if 'total_share' in hist_df.columns:
        hist_df['æ€»è‚¡æœ¬'] = hist_df['total_share'] * 1e4
    if 'float_share' in hist_df.columns:
        hist_df['æµé€šè‚¡æœ¬'] = hist_df['float_share'] * 1e4
    elif 'free_share' in hist_df.columns:
        hist_df['æµé€šè‚¡æœ¬'] = hist_df['free_share'] * 1e4

    # Reconstruct missing market values when necessary
    if 'æ€»å¸‚å€¼' not in hist_df.columns or hist_df['æ€»å¸‚å€¼'].isna().all():
        if 'æ”¶ç›˜' in hist_df.columns and 'æ€»è‚¡æœ¬' in hist_df.columns:
            hist_df['æ€»å¸‚å€¼'] = hist_df['æ”¶ç›˜'] * hist_df['æ€»è‚¡æœ¬']
    if 'æµé€šå¸‚å€¼' not in hist_df.columns or hist_df['æµé€šå¸‚å€¼'].isna().all():
        if 'æ”¶ç›˜' in hist_df.columns and 'æµé€šè‚¡æœ¬' in hist_df.columns:
            hist_df['æµé€šå¸‚å€¼'] = hist_df['æ”¶ç›˜'] * hist_df['æµé€šè‚¡æœ¬']

    needed_cols = ['æ—¥æœŸ', 'åç§°']
    for col in ['æµé€šå¸‚å€¼', 'æ€»å¸‚å€¼', 'æ”¶ç›˜', 'æ€»è‚¡æœ¬', 'æµé€šè‚¡æœ¬']:
        if col in hist_df.columns:
            needed_cols.append(col)
    return hist_df[needed_cols]

# Function to test available Tushare functions for historical data
def test_available_history_functions():
    """æµ‹è¯•å…³é”®çš„ Tushare æ•°æ®æ¥å£å¯ç”¨æ€§"""
    results = {}
    try:
        client = get_tushare_client()
    except RuntimeError as token_err:
        results["tushare_client"] = {
            "status": "error",
            "error": str(token_err),
        }
        return results

    checks = [
        {
            "name": "stock_basic",
            "callable": lambda: client.stock_basic(
                exchange="", list_status="L", fields="ts_code,name"
            ),
            "expected_column": "ts_code",
        },
        {
            "name": "daily_basic",
            "callable": lambda: client.daily_basic(
                ts_code="000001.SZ",
                start_date="20230101",
                end_date="20230105",
                fields="ts_code,trade_date,total_mv,circ_mv,close",
            ),
            "expected_column": "total_mv",
        },
    ]

    for item in checks:
        api_name = item["name"]
        try:
            df = call_tushare_api(item["callable"], api_label=api_name)
            if df is not None and not df.empty:
                result_entry = {
                    "status": "success",
                    "rows": len(df),
                    "columns": list(df.columns),
                    "sample": df.head(1).to_dict('records')[0],
                }
                expected_col = item.get("expected_column")
                if expected_col and expected_col not in df.columns:
                    result_entry["warning"] = f"ç¼ºå°‘ {expected_col} åˆ—"
                results[api_name] = result_entry
            else:
                results[api_name] = {
                    "status": "empty_result",
                    "columns": [],
                }
        except Exception as exc:
            results[api_name] = {
                "status": "error",
                "error": str(exc),
            }

    return results

# Function to get market value for specific dates
def get_market_value_for_dates(symbol, start_date, end_date, *, ts_code=None, stock_name=None):
    """è·å–æŒ‡å®šè‚¡ç¥¨åœ¨èµ·å§‹æ—¥å’Œç»“æŸæ—¥çš„å¸‚å€¼æ•°æ®"""
    try:
        df = get_stock_market_value(
            symbol,
            start_date,
            end_date,
            ts_code=ts_code,
            stock_name=stock_name,
        )
        if df is None or df.empty:
            return None, None, None, None, None, "å†å²è¡Œæƒ…ä¸ºç©º"
        
        # Ensure we have at least one record
        if len(df) < 1:
            return None, None, None, None, None, "å†å²è®°å½•æ•°é‡ä¸è¶³"
        
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        name = df.iloc[-1].get('åç§°', stock_name or symbol)
        circ_series = df['æµé€šå¸‚å€¼'] if 'æµé€šå¸‚å€¼' in df.columns else pd.Series(dtype=float)
        total_series = df['æ€»å¸‚å€¼'] if 'æ€»å¸‚å€¼' in df.columns else pd.Series(dtype=float)
        
        def extract_first_last(series):
            if series.empty:
                return None, None
            valid = series.dropna()
            if valid.empty:
                return None, None
            first_value = float(valid.iloc[0])
            last_value = float(valid.iloc[-1])
            return first_value, last_value
        
        start_circ_mv, end_circ_mv = extract_first_last(circ_series)
        start_total_mv, end_total_mv = extract_first_last(total_series)
        
        if start_total_mv is None or end_total_mv is None:
            # Attempt to reconstruct total market value from closing price and share count
            if 'æ”¶ç›˜' in df.columns and 'æ€»è‚¡æœ¬' in df.columns:
                reconstructed_total_series = (
                    pd.to_numeric(df['æ”¶ç›˜'], errors='coerce')
                    * pd.to_numeric(df['æ€»è‚¡æœ¬'], errors='coerce')
                )
                start_total_mv, end_total_mv = extract_first_last(reconstructed_total_series)

        if start_circ_mv is None or end_circ_mv is None:
            if 'æ”¶ç›˜' in df.columns and 'æµé€šè‚¡æœ¬' in df.columns:
                reconstructed_circ_series = (
                    pd.to_numeric(df['æ”¶ç›˜'], errors='coerce')
                    * pd.to_numeric(df['æµé€šè‚¡æœ¬'], errors='coerce')
                )
                start_circ_mv, end_circ_mv = extract_first_last(reconstructed_circ_series)
        
        if start_total_mv is None or end_total_mv is None:
            return None, None, None, None, None, "æ— æ³•è·å–æˆ–é‡å»ºæ€»å¸‚å€¼æ•°æ®"
        
        return name, start_circ_mv, end_circ_mv, start_total_mv, end_total_mv, None
    except Exception as e:
        return None, None, None, None, None, f"å¼‚å¸¸: {e}"

# Function to calculate date range formatted for API
def get_formatted_date_range(days_ago):
    """è®¡ç®—ä»å½“å‰æ—¥æœŸå‘å‰æ¨ç®—çš„æ—¥æœŸï¼Œæ ¼å¼åŒ–ä¸ºAPIæ‰€éœ€æ ¼å¼"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_ago)
    
    # Format dates to YYYYMMDD
    start_date_str = start_date.strftime('%Y%m%d')
    end_date_str = end_date.strftime('%Y%m%d')
    
    return start_date_str, end_date_str

# Function to test API with a single stock
def test_api_connectivity(start_date_str, end_date_str):
    """æµ‹è¯•ä¸APIçš„è¿æ¥å’Œæ•°æ®è·å–"""
    test_stocks = [
        {"code": "000001", "ts_code": "000001.SZ"},  # å¹³å®‰é“¶è¡Œ
        {"code": "600000", "ts_code": "600000.SH"},  # æµ¦å‘é“¶è¡Œ
    ]
    results = []
    
    # First, test which functions are available
    function_test_results = test_available_history_functions()
    
    # Now test actual data retrieval for specific stocks
    for stock in test_stocks:
        try:
            code = stock["code"]
            ts_code = stock.get("ts_code") or to_ts_code(code)
            hist_df = get_stock_market_value(
                code,
                start_date_str,
                end_date_str,
                ts_code=ts_code,
                stock_name=None,
            )
            
            if hist_df is not None and not hist_df.empty:
                sample_data = hist_df.head(1).to_dict('records')[0]
                results.append({
                    'stock': code,
                    'ts_code': ts_code,
                    'status': 'success',
                    'columns': list(hist_df.columns),
                    'sample': sample_data
                })
            else:
                results.append({
                    'stock': code,
                    'ts_code': ts_code,
                    'status': 'empty_response',
                    'columns': [],
                    'sample': {}
                })
        except Exception as e:
            results.append({
                'stock': stock["code"],
                'ts_code': stock.get("ts_code") or to_ts_code(stock["code"]),
                'status': 'error',
                'error': str(e),
                'traceback': traceback.format_exc()
            })
    
    return {
        'stock_tests': results,
        'function_tests': function_test_results
    }

# Main functionality
def main():
    # Sidebar options
    st.sidebar.title("åˆ†æè®¾ç½®")
    
    today = datetime.now().date()
    default_start = today - timedelta(days=DEFAULT_LOOKBACK_DAYS)
    
    start_date = st.sidebar.date_input(
        "å¼€å§‹æ—¥æœŸ",
        value=default_start,
        max_value=today
    )
    
    end_date = st.sidebar.date_input(
        "ç»“æŸæ—¥æœŸ",
        value=today,
        min_value=start_date,
        max_value=today
    )
    
    if start_date > end_date:
        st.sidebar.error("ç»“æŸæ—¥æœŸéœ€æ™šäºæˆ–ç­‰äºå¼€å§‹æ—¥æœŸ")
        return
    
    start_date_str = start_date.strftime('%Y%m%d')
    end_date_str = end_date.strftime('%Y%m%d')
    
    
    
    # Start analysis button
    if st.sidebar.button("å¼€å§‹åˆ†æ", type="primary"):
        try:
            # Display selected parameters
            st.subheader("åˆ†æå‚æ•°")
            st.write(f"- æ—¶é—´åŒºé—´: {start_date_str} è‡³ {end_date_str}")
            st.write("- æ’é™¤ ST/*ST è‚¡ç¥¨ä»¥é¿å…å¼‚å¸¸æ•°æ®")
            st.write(f"- å›ºå®šè¾“å‡ºæ€»å¸‚å€¼æ¶¨è·Œæ¦œ TOP/BOTTOM {TOP_COUNT}")
            
            # Get stock list
            with st.spinner("æ­£åœ¨è·å– A è‚¡è‚¡ç¥¨åˆ—è¡¨..."):
                stock_list_df = get_stock_list()
                if stock_list_df.empty:
                    st.error("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·ç¨åé‡è¯•")
                    return
                stock_list_df['code'] = stock_list_df['code'].astype(str).str.zfill(6)
                if 'ts_code' in stock_list_df.columns:
                    stock_list_df['ts_code'] = stock_list_df['ts_code'].fillna("").astype(str)
                    missing_ts = stock_list_df['ts_code'].eq("")
                    stock_list_df.loc[missing_ts, 'ts_code'] = stock_list_df.loc[missing_ts, 'code'].apply(to_ts_code)
                else:
                    stock_list_df['ts_code'] = stock_list_df['code'].apply(to_ts_code)
                stock_list_df = stock_list_df.drop_duplicates(subset=['code'])
                initial_count = len(stock_list_df)
                stock_list_df = stock_list_df[~stock_list_df['name'].astype(str).str.contains('ST', case=False, na=False)]
                removed = initial_count - len(stock_list_df)
                if removed > 0:
                    st.info(f"å·²è‡ªåŠ¨æ’é™¤ {removed} åª ST/*ST è‚¡ç¥¨")
                
                st.success(f"å…±è·å–åˆ° {len(stock_list_df)} åª A è‚¡è‚¡ç¥¨")
            
            if stock_list_df.empty:
                st.warning("ç­›é€‰åæ²¡æœ‰å¯ç”¨è‚¡ç¥¨ï¼Œè¯·è°ƒæ•´æ¡ä»¶åé‡è¯•")
                return
            
            stock_records = stock_list_df.to_dict('records')
            total_stocks = len(stock_records)

            try:
                with open(ERROR_LOG_FILE, "w", encoding="utf-8") as log_file:
                    log_file.write(f"=== å¸‚å€¼æ’åºè¿è¡Œå¼€å§‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                    log_file.write(f"æ—¶é—´åŒºé—´: {start_date_str} è‡³ {end_date_str}\n")
                    log_file.write(f"è‚¡ç¥¨æ€»æ•°: {total_stocks}\n")
            except Exception:
                pass
            
            progress_bar = st.progress(0.0)
            status_text = st.empty()
            results = []
            error_logs = []
            
            def process_stock(record):
                code = str(record.get('code')).zfill(6)
                fallback_name = record.get('name') or code
                ts_code = record.get('ts_code') or to_ts_code(code)
                try:
                    name, _, _, start_total_mv, end_total_mv, reason = get_market_value_for_dates(
                        code,
                        start_date_str,
                        end_date_str,
                        ts_code=ts_code,
                        stock_name=fallback_name,
                    )
                    stock_name = name or fallback_name
                    if reason:
                        message = f"è‚¡ç¥¨ {code} ({stock_name}) æ€»å¸‚å€¼æ•°æ®ç¼ºå¤±: {reason}"
                        append_error_log(message)
                        return None, message
                    change = end_total_mv - start_total_mv
                    if pd.notna(start_total_mv) and start_total_mv != 0:
                        change_percent = change / start_total_mv * 100
                    else:
                        change_percent = np.nan
                    return {
                        "code": code,
                        "name": stock_name,
                        "start_total_mv": float(start_total_mv),
                        "end_total_mv": float(end_total_mv),
                        "total_mv_change": float(change),
                        "total_mv_change_percent": float(change_percent) if not pd.isna(change_percent) else np.nan
                    }, None
                except Exception as exc:
                    message = f"è‚¡ç¥¨ {code} ({fallback_name}) å¼‚å¸¸: {exc}"
                    append_error_log(message)
                    return None, message
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_record = {
                    executor.submit(process_stock, record): record for record in stock_records
                }
                for idx, future in enumerate(as_completed(future_to_record), start=1):
                    record = future_to_record[future]
                    code = str(record.get('code')).zfill(6)
                    fallback_name = record.get('name') or code
                    try:
                        data, error = future.result()
                        if data:
                            results.append(data)
                        if error:
                            error_logs.append(error)
                            append_error_log(error)
                    except Exception as exc:
                        message = f"è‚¡ç¥¨ {code} ({fallback_name}) çº¿ç¨‹å¼‚å¸¸: {exc}"
                        error_logs.append(message)
                        append_error_log(message)
                    progress_bar.progress(min(idx / total_stocks, 1.0))
                    status_text.text(
                        f"å·²å¤„ç†: {idx}/{total_stocks} | æœ‰æ•ˆ: {len(results)} | é”™è¯¯: {len(error_logs)}"
                    )
            
            progress_bar.progress(1.0)
            
            with st.expander("é”™è¯¯æ—¥å¿— (å±•å¼€æŸ¥çœ‹è¯¦æƒ…)", expanded=False):
                if error_logs:
                    st.write(f"å¤„ç†è¿‡ç¨‹ä¸­é‡åˆ° {len(error_logs)} ä¸ªå¼‚å¸¸ï¼š")
                    for i, log in enumerate(error_logs[:ERROR_LOG_DISPLAY_LIMIT], start=1):
                        st.text(f"{i}. {log}")
                    remaining = len(error_logs) - ERROR_LOG_DISPLAY_LIMIT
                    if remaining > 0:
                        st.text(f"... è¿˜æœ‰ {remaining} æ¡é”™è¯¯æœªæ˜¾ç¤º")
                    st.caption(f"å®Œæ•´é”™è¯¯è¯¦æƒ…å·²å†™å…¥ {ERROR_LOG_FILE}")
                else:
                    st.write("å¤„ç†è¿‡ç¨‹æœªå‘ç°é”™è¯¯")
            
            if not results:
                st.error("æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®ï¼Œè¯·å°è¯•å…¶ä»–æ—¶é—´åŒºé—´æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
                return
            
            results_df = pd.DataFrame(results)
            if results_df.empty:
                st.error("æš‚æ— å¯ç”¨çš„å¸‚å€¼å˜åŒ–ç»“æœ")
                return
            
            available_count = len(results_df)
            st.info(f"æˆåŠŸæ”¶é›†äº† {available_count} åªè‚¡ç¥¨çš„æ€»å¸‚å€¼æ•°æ®")
            
            top_k = min(TOP_COUNT, available_count)
            if top_k == 0:
                st.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºæ’åå±•ç¤º")
                return
            
            top_increase = results_df.nlargest(top_k, 'total_mv_change').copy()
            top_decrease = results_df.nsmallest(top_k, 'total_mv_change').copy()
            
            for df in (top_increase, top_decrease):
                df['start_total_mv_äº¿'] = df['start_total_mv'] / 1e8
                df['end_total_mv_äº¿'] = df['end_total_mv'] / 1e8
                df['change_total_mv_äº¿'] = df['total_mv_change'] / 1e8
                df['change_percent'] = df['total_mv_change_percent']
            
            display_columns = [
                'code', 'name', 'start_total_mv_äº¿', 'end_total_mv_äº¿', 'change_total_mv_äº¿', 'change_percent'
            ]
            
            display_increase = top_increase[display_columns].copy()
            display_increase.columns = [
                'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)', 'ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)', 'æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)', 'å˜åŒ–ç™¾åˆ†æ¯”(%)'
            ]
            display_increase['èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)'] = display_increase['èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)'].round(2)
            display_increase['ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)'] = display_increase['ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)'].round(2)
            display_increase['æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)'] = display_increase['æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)'].round(2)
            display_increase['å˜åŒ–ç™¾åˆ†æ¯”(%)'] = display_increase['å˜åŒ–ç™¾åˆ†æ¯”(%)'].round(2)
            
            display_decrease = top_decrease[display_columns].copy()
            display_decrease.columns = [
                'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)', 'ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)', 'æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)', 'å˜åŒ–ç™¾åˆ†æ¯”(%)'
            ]
            display_decrease['èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)'] = display_decrease['èµ·å§‹æ€»å¸‚å€¼(äº¿å…ƒ)'].round(2)
            display_decrease['ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)'] = display_decrease['ç»“æŸæ€»å¸‚å€¼(äº¿å…ƒ)'].round(2)
            display_decrease['æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)'] = display_decrease['æ€»å¸‚å€¼å˜åŒ–(äº¿å…ƒ)'].round(2)
            display_decrease['å˜åŒ–ç™¾åˆ†æ¯”(%)'] = display_decrease['å˜åŒ–ç™¾åˆ†æ¯”(%)'].round(2)
            
            st.subheader(f"æ€»å¸‚å€¼å¢åŠ æœ€å¤šçš„å‰ {top_k} åªè‚¡ç¥¨")
            st.dataframe(display_increase, use_container_width=True)
            
            st.subheader(f"æ€»å¸‚å€¼å‡å°‘æœ€å¤šçš„å‰ {top_k} åªè‚¡ç¥¨")
            st.dataframe(display_decrease, use_container_width=True)
            
            st.subheader("æ•°æ®ä¸‹è½½")
            csv_increase = display_increase.to_csv(index=False)
            csv_decrease = display_decrease.to_csv(index=False)
            col1, col2 = st.columns(2)
            with col1:
                st.download_button(
                    label="ä¸‹è½½æ€»å¸‚å€¼å¢åŠ æ¦œå•",
                    data=csv_increase,
                    file_name=f"top_increase_total_mv_{start_date_str}_to_{end_date_str}.csv",
                    mime="text/csv"
                )
            with col2:
                st.download_button(
                    label="ä¸‹è½½æ€»å¸‚å€¼å‡å°‘æ¦œå•",
                    data=csv_decrease,
                    file_name=f"top_decrease_total_mv_{start_date_str}_to_{end_date_str}.csv",
                    mime="text/csv"
                )
        except Exception as e:
            st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.code(traceback.format_exc())

if __name__ == "__main__":
    main()
